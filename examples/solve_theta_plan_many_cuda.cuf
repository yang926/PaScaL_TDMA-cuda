!======================================================================================================================
!> @file        solve_theta_plan_many.f90
!> @brief       This file contains a solver subroutine for the example problem of PaScaL_TDMA.
!> @details     The target example problem is the three-dimensional time-dependent heat conduction problem 
!>              in a unit cube domain applied with the boundary conditions of vertically constant temperature 
!>              and horizontally periodic boundaries.
!> @author      
!>              - Kiha Kim (k-kiha@yonsei.ac.kr), Department of Computational Science & Engineering, Yonsei University
!>              - Ji-Hoon Kang (jhkang@kisti.re.kr), Korea Institute of Science and Technology Information
!>              - Jung-Il Choi (jic@yonsei.ac.kr), Department of Computational Science & Engineering, Yonsei University
!>
!> @date        June 2019
!> @version     1.0
!> @par         Copyright
!>              Copyright (c) 2019 Kiha Kim and Jung-Il choi, Yonsei University and 
!>              Ji-Hoon Kang, Korea Institute of Science and Technology Information, All rights reserved.
!> @par         License     
!>              This project is release under the terms of the MIT License (see LICENSE in )
!======================================================================================================================
module solve_theta_cuda

    integer, parameter, private :: BLOCK_DIM_X=4, BLOCK_DIM_Y=4, BLOCK_DIM_Z=4

contains
    !>
    !>
    !> @brief       An example solver for many tridiagonal systems of equations using PaScaL_TDMA.
    !> @details     This subroutine is for many tridiagonal systems of equations.
    !>              It solves the the three-dimensional time-dependent heat conduction problem using PaScaL_TDMA.
    !>              PaScaL_TDMA plans are created for many tridiagonal systems of equations and
    !>              the many tridiagonal systems are solved plane-by-plane.
    !> @param       theta       Main 3-D variable to be solved
    !>
    subroutine solve_theta_plan_many_cuda(theta)

        use omp_lib
        use mpi
        use global
        use mpi_subdomain
        use mpi_topology
        use PaScaL_TDMA_cuda
        use cudafor
       ! use nvtx

        implicit none

        double precision, dimension(0:nx_sub, 0:ny_sub, 0:nz_sub), intent(inout) :: theta

        type(ptdma_plan_many_cuda)  :: px_many, pz_many , py_many  ! Plan for many tridiagonal systems of equations

        integer :: i, j, k
        integer :: myrank, ierr, request(12), istat
        integer :: time_step        ! Current time step
        double precision :: t_curr  ! Current simulation time
        double precision :: time_a, time_b, time_t_TDMA=0.0d0
        double precision :: time_1, time_2, time_3, time_4, time_5, time_6
        double precision :: time_t_1, time_t_2, time_t_3, time_t_4, time_t_5, time_t_6
    
        ! Temporary variables for coefficient computations
        double precision :: Ct_half_over_dx, Ct_half_over_dy, Ct_half_over_dz
        double precision, allocatable, dimension(:,:,:) :: rhs            ! r.h.s. array

        double precision, allocatable, dimension(:, :, :), device   :: theta_d, rhs_d           ! r.h.s. array
        double precision, allocatable, target, dimension(:), device :: ap_d, am_d, ac_d, ad_d           ! Coefficient of tridiagonal matrix
        double precision, pointer, dimension(:,:,:), device         :: ap_ptr, am_ptr, ac_ptr, ad_ptr   ! Coefficient of tridiagonal matrix

        double precision, device    :: dmz_sub_d(0:nz_sub), dmy_sub_d(0:ny_sub), dmx_sub_d(0:nx_sub)
        double precision, device    :: thetaBC3_sub_d(0:nx_sub, 0:nz_sub), thetaBC4_sub_d(0:nx_sub, 0:nz_sub)
        integer, device             :: jpbc_index_d(0:ny_sub), jmbc_index_d(0:ny_sub)

        ! Block and thread dimension
        integer, parameter  :: thread_in_x = BLOCK_DIM_X, thread_in_y = BLOCK_DIM_Y, thread_in_z = BLOCK_DIM_Z
        integer             :: block_in_x, block_in_y, block_in_z
        type(dim3)          :: blocks, threads

        ! Communication buffer
        double precision, allocatable, dimension(:,:)   :: sbuf_x0(:,:), sbuf_x1(:,:), sbuf_y0(:,:), sbuf_y1(:,:), sbuf_z0(:,:), sbuf_z1(:,:)
        double precision, allocatable, dimension(:,:)   :: rbuf_x0(:,:), rbuf_x1(:,:), rbuf_y0(:,:), rbuf_y1(:,:), rbuf_z0(:,:), rbuf_z1(:,:)
        double precision, allocatable, dimension(:,:), device   :: dbuf_x0(:,:), dbuf_x1(:,:), dbuf_y0(:,:), dbuf_y1(:,:), dbuf_z0(:,:), dbuf_z1(:,:)

        call MPI_Comm_rank( MPI_COMM_WORLD, myrank, ierr)
    
        Ct_half_over_dx = 0.5d0*Ct/dx
        Ct_half_over_dy = 0.5d0*Ct/dy
        Ct_half_over_dz = 0.5d0*Ct/dz

        dmx_sub_d = dmx_sub
        dmy_sub_d = dmy_sub
        dmz_sub_d = dmz_sub
        jpbc_index_d = jpbc_index
        jmbc_index_d = jmbc_index
        thetaBC3_sub_d = thetaBC3_sub
        thetaBC4_sub_d = thetaBC4_sub

        ! Calculating r.h.s.
        allocate( rhs_d(nx_sub-1, ny_sub-1, nz_sub-1))
        allocate( theta_d(0:nx_sub, 0:ny_sub, 0:nz_sub))

        allocate( sbuf_x0(ny_sub-1,nz_sub-1), sbuf_x1(ny_sub-1,nz_sub-1) )
        allocate( sbuf_y0(nx_sub-1,nz_sub-1), sbuf_y1(nx_sub-1,nz_sub-1) )
        allocate( sbuf_z0(nx_sub-1,ny_sub-1), sbuf_z1(nx_sub-1,ny_sub-1) )
        allocate( rbuf_x0(ny_sub-1,nz_sub-1), rbuf_x1(ny_sub-1,nz_sub-1) )
        allocate( rbuf_y0(nx_sub-1,nz_sub-1), rbuf_y1(nx_sub-1,nz_sub-1) )
        allocate( rbuf_z0(nx_sub-1,ny_sub-1), rbuf_z1(nx_sub-1,ny_sub-1) )
        allocate( dbuf_x0(ny_sub-1,nz_sub-1), dbuf_x1(ny_sub-1,nz_sub-1) )
        allocate( dbuf_y0(nx_sub-1,nz_sub-1), dbuf_y1(nx_sub-1,nz_sub-1) )
        allocate( dbuf_z0(nx_sub-1,ny_sub-1), dbuf_z1(nx_sub-1,ny_sub-1) )

        sbuf_x0 = 0.0d0; sbuf_x1 = 0.0d0
        sbuf_y0 = 0.0d0; sbuf_y1 = 0.0d0
        sbuf_z0 = 0.0d0; sbuf_z1 = 0.0d0

        rbuf_x0 = 0.0d0; rbuf_x1 = 0.0d0
        rbuf_y0 = 0.0d0; rbuf_y1 = 0.0d0
        rbuf_z0 = 0.0d0; rbuf_z1 = 0.0d0

        ! Setting the thread and block
        block_in_x = (nx_sub-1)/thread_in_x
        if(block_in_x.eq.0 .or. mod((nx_sub-1), thread_in_x)) then
            print '(a,i5,a,i5)', '[Error] ny_sub-1 should be a multiple of thread_in_x. thread_in_x = ',thread_in_x,', nx_sub-1 = ',nx_sub-1
            call MPI_Finalize(ierr)
            stop
        endif

        block_in_y = (ny_sub-1)/thread_in_y
        if(block_in_y.eq.0 .or. mod((ny_sub-1), thread_in_y)) then
            print '(a,i5,a,i5)', '[Error] nz_sub-1 should be a multiple of thread_in_y. thread_in_y = ',thread_in_y,', ny_sub-1 = ',ny_sub-1
            call MPI_Finalize(ierr)
            stop
        endif

        block_in_z = (nz_sub-1)/thread_in_z
        if(block_in_z.eq.0 .or. mod((nz_sub-1), thread_in_z)) then
            print '(a,i5,a,i5)', '[Error] nz_sub-1 should be a multiple of thread_in_z. thread_in_z = ',thread_in_z,', nz_sub-1 = ',nz_sub-1
            call MPI_Finalize(ierr)
            stop
        endif

        threads = dim3(thread_in_x, thread_in_y, thread_in_z)
        blocks  = dim3(block_in_x, block_in_y, block_in_z)
    
        ! Simulation begins
        t_curr = tStart
        dt = dtstart

        ! Create a PaScaL_TDMA plan for the tridiagonal systems.
        call PaScaL_TDMA_plan_many_create_cuda(pz_many, nx_sub-1, nz_sub-1, ny_sub-1, comm_1d_z%myrank, comm_1d_z%nprocs, comm_1d_z%mpi_comm)
        call PaScaL_TDMA_plan_many_create_cuda(py_many, nx_sub-1, ny_sub-1, nz_sub-1, comm_1d_y%myrank, comm_1d_y%nprocs, comm_1d_y%mpi_comm)
        call PaScaL_TDMA_plan_many_create_cuda(px_many, ny_sub-1, nx_sub-1, nz_sub-1, comm_1d_x%myrank, comm_1d_x%nprocs, comm_1d_x%mpi_comm)

        allocate( ap_d((nx_sub-1)*(ny_sub-1)*(nz_sub-1)) )
        allocate( ac_d((nx_sub-1)*(ny_sub-1)*(nz_sub-1)) )
        allocate( am_d((nx_sub-1)*(ny_sub-1)*(nz_sub-1)) )
        allocate( ad_d((nx_sub-1)*(ny_sub-1)*(nz_sub-1)) )

        !$cuf kernel do <<< *,* >>>
        do i = 1, (nx_sub-1)*(ny_sub-1)*(nz_sub-1)
            am_d(i)=0.0d0
            ac_d(i)=0.0d0
            ap_d(i)=0.0d0
            ad_d(i)=0.0d0
        enddo
    
        theta_d = theta

        do time_step = 1, Tmax

            t_curr = t_curr + dt
            if(myrank==0) write(*,*) '[Main] Current time step = ', time_step
        
            call build_RHS_cuda<<<blocks, threads>>>(theta_d, rhs_d, dmx_sub_d, dmy_sub_d, dmz_sub_d, jpbc_index_d, jmbc_index_d, thetaBC3_sub_d, thetaBC4_sub_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dx, Ct_half_over_dy, Ct_half_over_dz)

            ! solve in the z-direction.
            ap_ptr(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1) => ap_d
            ac_ptr(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1) => ac_d
            am_ptr(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1) => am_d
            ad_ptr(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1) => ad_d

            ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
            call build_LHSz_cuda<<<blocks, threads>>>(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmz_sub_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dz, dt)
            !Solve the tridiagonal systems under the defined plan with periodic boundary conditions.
            call PaScaL_TDMA_many_solve_cycle_cuda(pz_many, am_ptr, ac_ptr, ap_ptr, ad_ptr, nx_sub-1, nz_sub-1, ny_sub-1)
            ! Return the solution to the r.h.s. plane-by-plane
            call transpose_ijk2ijk<<<blocks, threads>>>(ad_ptr, rhs_d, nx_sub, ny_sub, nz_sub)
            nullify( ap_ptr, ac_ptr, am_ptr, ad_ptr )
            ! solve in the y-direction.
            ap_ptr(1:nx_sub-1, 1:nz_sub-1, 1:ny_sub-1) => ap_d
            ac_ptr(1:nx_sub-1, 1:nz_sub-1, 1:ny_sub-1) => ac_d
            am_ptr(1:nx_sub-1, 1:nz_sub-1, 1:ny_sub-1) => am_d
            ad_ptr(1:nx_sub-1, 1:nz_sub-1, 1:ny_sub-1) => ad_d

            ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
            call build_LHSy_cuda<<<blocks, threads>>>(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmy_sub_d, jpbc_index_d, jmbc_index_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dy, dt)

            ! Solve the tridiagonal systems under the defined plan.
            ! Test area of Pascal_TDMA_many_solve_cuda
            ierr = cudaDeviceSynchronize()
            time_a=mpi_wtime()

            call PaScaL_TDMA_many_solve_cuda(py_many, am_ptr, ac_ptr, ap_ptr, ad_ptr, nx_sub-1, ny_sub-1, nz_sub-1, time_1, time_2, time_3, time_4, time_5, time_6)
            ierr = cudaDeviceSynchronize()
            time_b=mpi_wtime()

            if(time_step.ge.2) then 
        ! if(myrank==0) write(*,*) "t Time", time_b-time_a
                time_t_TDMA=time_t_TDMA*dble(time_step-2)/dble(time_step-1)+(time_b-time_a)/dble(time_step-1)

                time_t_1=time_t_1*dble(time_step-2)/dble(time_step-1)+(time_1)/dble(time_step-1)
                time_t_2=time_t_2*dble(time_step-2)/dble(time_step-1)+(time_2)/dble(time_step-1)
                time_t_3=time_t_3*dble(time_step-2)/dble(time_step-1)+(time_3)/dble(time_step-1)
                time_t_4=time_t_4*dble(time_step-2)/dble(time_step-1)+(time_4)/dble(time_step-1)
                time_t_5=time_t_5*dble(time_step-2)/dble(time_step-1)+(time_5)/dble(time_step-1)
                time_t_6=time_t_6*dble(time_step-2)/dble(time_step-1)+(time_6)/dble(time_step-1)
            endif

            ! Return the solution to the r.h.s. plane-by-plane.
            call transpose_ikj2ijk<<<blocks, threads>>>(ad_ptr, rhs_d, nx_sub, ny_sub, nz_sub)

            nullify( ap_ptr, ac_ptr, am_ptr, ad_ptr )

            ! solve in the x-direction.
            ap_ptr(1:nz_sub-1, 1:ny_sub-1, 1:nx_sub-1) => ap_d
            ac_ptr(1:nz_sub-1, 1:ny_sub-1, 1:nx_sub-1) => ac_d
            am_ptr(1:nz_sub-1, 1:ny_sub-1, 1:nx_sub-1) => am_d
            ad_ptr(1:nz_sub-1, 1:ny_sub-1, 1:nx_sub-1) => ad_d

            ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
            call build_LHSx_cuda<<<blocks, threads>>>(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmx_sub_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dx, dt)

            ! Solve the tridiagonal systems under the defined plan with periodic boundary conditions.
            call PaScaL_TDMA_many_solve_cycle_cuda(px_many, am_ptr, ac_ptr, ap_ptr, ad_ptr, ny_sub-1, nx_sub-1, nz_sub-1)


            ! Return the solution to theta plane-by-plane.
            call update_theta_cuda<<<blocks, threads>>>(ad_ptr, theta_d, nx_sub, ny_sub, nz_sub)

            nullify( ap_ptr, ac_ptr, am_ptr, ad_ptr )
            ! Update ghostcells from the solution.
            if(comm_1d_x%west_rank.ne.MPI_PROC_NULL) then

                !$cuf kernel do <<< *,* >>>
                do k = 1, nz_sub-1
                    do j = 1, ny_sub-1
                        dbuf_x0(j,k) = theta_d(1,j,k)
                    enddo
                enddo

                sbuf_x0 = dbuf_x0
            endif

            if(comm_1d_x%east_rank.ne.MPI_PROC_NULL) then

                !$cuf kernel do <<<  *,* >>>
                do k = 1, nz_sub-1
                    do j = 1, ny_sub-1
                        dbuf_x1(j,k) = theta_d(nx_sub-1,j,k)
                    enddo
                enddo

                sbuf_x1 = dbuf_x1
            endif
                
            if(comm_1d_y%west_rank.ne.MPI_PROC_NULL) then
                !$cuf kernel do <<< *,* >>>
                do k = 1, nz_sub-1
                    do i = 1, nx_sub-1
                        dbuf_y0(i,k) = theta_d(i,1,k)
                    enddo
                enddo

                sbuf_y0 = dbuf_y0
            endif

            if(comm_1d_y%east_rank.ne.MPI_PROC_NULL) then
                !$cuf kernel do <<<  *,* >>>
                do k = 1, nz_sub-1
                    do i = 1, nx_sub-1
                        dbuf_y1(i,k) = theta_d(i,ny_sub-1,k)
                    enddo
                enddo

                sbuf_y1 = dbuf_y1
            endif

            if(comm_1d_z%west_rank.ne.MPI_PROC_NULL) then
                !$cuf kernel do <<< *,* >>>
                do j = 1, ny_sub-1
                    do i = 1, nx_sub-1
                        dbuf_z0(i,j) = theta_d(i,j,1)
                    enddo
                enddo

                sbuf_z0 = dbuf_z0
            endif

            if(comm_1d_z%east_rank.ne.MPI_PROC_NULL) then
                !$cuf kernel do <<<  *,* >>>
                do j = 1, ny_sub-1
                    do i = 1, nx_sub-1
                        dbuf_z1(i,j) = theta_d(i,j,nz_sub-1)
                    enddo
                enddo

                sbuf_z1 = dbuf_z1
            endif
            call MPI_Isend(sbuf_x0, (ny_sub-1)*(nz_sub-1), MPI_DOUBLE_PRECISION, comm_1d_x%west_rank, 111, comm_1d_x%mpi_comm, request(1), ierr)
            call MPI_Irecv(rbuf_x0, (ny_sub-1)*(nz_sub-1), MPI_DOUBLE_PRECISION, comm_1d_x%west_rank, 222, comm_1d_x%mpi_comm, request(2), ierr)
            call MPI_Isend(sbuf_x1, (ny_sub-1)*(nz_sub-1), MPI_DOUBLE_PRECISION, comm_1d_x%east_rank, 222, comm_1d_x%mpi_comm, request(3), ierr)
            call MPI_Irecv(rbuf_x1, (ny_sub-1)*(nz_sub-1), MPI_DOUBLE_PRECISION, comm_1d_x%east_rank, 111, comm_1d_x%mpi_comm, request(4), ierr)
            call MPI_Isend(sbuf_y0, (nx_sub-1)*(nz_sub-1), MPI_DOUBLE_PRECISION, comm_1d_y%west_rank, 111, comm_1d_y%mpi_comm, request(5), ierr)
            call MPI_Irecv(rbuf_y0, (nx_sub-1)*(nz_sub-1), MPI_DOUBLE_PRECISION, comm_1d_y%west_rank, 222, comm_1d_y%mpi_comm, request(6), ierr)
            call MPI_Irecv(rbuf_y1, (nx_sub-1)*(nz_sub-1), MPI_DOUBLE_PRECISION, comm_1d_y%east_rank, 111, comm_1d_y%mpi_comm, request(7), ierr)
            call MPI_Isend(sbuf_y1, (nx_sub-1)*(nz_sub-1), MPI_DOUBLE_PRECISION, comm_1d_y%east_rank, 222, comm_1d_y%mpi_comm, request(8), ierr)
            call MPI_Isend(sbuf_z0, (nx_sub-1)*(ny_sub-1), MPI_DOUBLE_PRECISION, comm_1d_z%west_rank, 111, comm_1d_z%mpi_comm, request(9), ierr)
            call MPI_Irecv(rbuf_z0, (nx_sub-1)*(ny_sub-1), MPI_DOUBLE_PRECISION, comm_1d_z%west_rank, 222, comm_1d_z%mpi_comm, request(10), ierr)
            call MPI_Irecv(rbuf_z1, (nx_sub-1)*(ny_sub-1), MPI_DOUBLE_PRECISION, comm_1d_z%east_rank, 111, comm_1d_z%mpi_comm, request(11), ierr)
            call MPI_Isend(sbuf_z1, (nx_sub-1)*(ny_sub-1), MPI_DOUBLE_PRECISION, comm_1d_z%east_rank, 222, comm_1d_z%mpi_comm, request(12), ierr)
            call MPI_Waitall(12, request, MPI_STATUSES_IGNORE, ierr)
            if(comm_1d_x%west_rank.ne.MPI_PROC_NULL) then
                dbuf_x0 = rbuf_x0

                !$cuf kernel do <<<  *,* >>>
                do k = 1, nz_sub-1
                    do j = 1, ny_sub-1
                        theta_d(0,j,k) = dbuf_x0(j,k)
                    enddo
                enddo
            endif

            if(comm_1d_x%east_rank.ne.MPI_PROC_NULL) then
                
                dbuf_x1 = rbuf_x1

                !$cuf kernel do <<<  *,* >>>
                do k = 1, nz_sub-1
                    do j = 1, ny_sub-1
                        theta_d(nx_sub,j,k) = dbuf_x1(j,k)
                    enddo
                enddo
            endif

            if(comm_1d_y%west_rank.ne.MPI_PROC_NULL) then
                dbuf_y0 = rbuf_y0
                !$cuf kernel do <<<  *,* >>>
                do k = 1, nz_sub-1
                    do i = 1, nx_sub-1
                        theta_d(i,0,k) = dbuf_y0(i,k)
                    enddo
                enddo
            endif

            if(comm_1d_y%east_rank.ne.MPI_PROC_NULL) then
                dbuf_y1 = rbuf_y1

                !$cuf kernel do <<<  *,* >>>
                do k = 1, nz_sub-1
                    do i = 1, nx_sub-1
                        theta_d(i,ny_sub,k) = dbuf_y1(i,k)
                    enddo
                enddo
            endif

            if(comm_1d_z%west_rank.ne.MPI_PROC_NULL) then
                dbuf_z0 = rbuf_z0

                !$cuf kernel do <<<  *,* >>>
                do j = 1, ny_sub-1
                    do i = 1, nx_sub-1
                        theta_d(i,j,0) = dbuf_z0(i,j)
                    enddo
                enddo
            endif

            if(comm_1d_z%east_rank.ne.MPI_PROC_NULL) then
                dbuf_z1 = rbuf_z1

                !$cuf kernel do <<<  *,* >>>
                do j = 1, ny_sub-1
                    do i = 1, nx_sub-1
                        theta_d(i,j,nz_sub) = dbuf_z1(i,j)
                    enddo
                enddo
            endif

        end do
        if(myrank==0) write(*,*) "Time", time_t_TDMA
        if(myrank==0) write(*,'(1A15,6E15.5)') "Time_section", time_t_1, time_t_2, time_t_3, time_t_4, time_t_5, time_t_6

        theta = theta_d
        ! Destroy the PaScaL_TDMA plan for the tridiagonal systems.
        call PaScaL_TDMA_plan_many_destroy_cuda(pz_many,comm_1d_z%nprocs)
        call PaScaL_TDMA_plan_many_destroy_cuda(px_many,comm_1d_x%nprocs)
        call PaScaL_TDMA_plan_many_destroy_cuda(py_many,comm_1d_y%nprocs)

        deallocate( ap_d, am_d, ac_d, ad_d )
        deallocate( rhs_d, theta_d )

        deallocate( sbuf_x0, sbuf_x1 )
        deallocate( sbuf_y0, sbuf_y1 )
        deallocate( sbuf_z0, sbuf_z1 )
        deallocate( rbuf_x0, rbuf_x1 )
        deallocate( rbuf_y0, rbuf_y1 )
        deallocate( rbuf_z0, rbuf_z1 )
        deallocate( dbuf_x0, dbuf_x1 )
        deallocate( dbuf_y0, dbuf_y1 )
        deallocate( dbuf_z0, dbuf_z1 )

    end subroutine solve_theta_plan_many_cuda

    !>
    !> @brief   Modified Thomas algorithm : Update solution
    !> @param   plan        Plan for many tridiagonal systems of equations
    !> @param   A           Coefficients in lower diagonal elements
    !> @param   C           Coefficients in upper diagonal elements
    !> @param   D           Coefficients in right-hand side terms
    !> @param   n_sys       Number of tridiagonal systems per process
    !> @param   n_row       Number of rows in each process, size of a tridiagonal matrix N divided by nprocs
    !>
    attributes(global) subroutine build_RHS_cuda(theta_d, rhs_d, dmx_sub_d, dmy_sub_d, dmz_sub_d, jpbc_index_d, jmbc_index_d, thetaBC3_sub_d, thetaBC4_sub_d, nx_sub, ny_sub, nz_sub, coefx, coefy, coefz)

        implicit none

        double precision, device, intent(in)    :: theta_d(0:nx_sub, 0:ny_sub, 0:nz_sub)          ! r.h.s. array
        double precision, device, intent(inout) :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device :: dmz_sub_d(0:nz_sub), dmy_sub_d(0:ny_sub), dmx_sub_d(0:nx_sub)
        double precision, device :: thetaBC3_sub_d(0:nx_sub, 0:nz_sub), thetaBC4_sub_d(0:nx_sub, 0:nz_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefx, coefy, coefz
        integer, device :: jpbc_index_d(0:ny_sub), jmbc_index_d(0:ny_sub)

        ! Loop and index variables
        integer :: i,j,k
        ! integer :: ip, jp, kp
        ! integer :: im, jm, km
        integer :: jem, jep
        integer :: ti, tj, tk

        ! Temporary memory for theta
        ! double precision, shared    :: theta_block(0:BLOCK_DIM_X+1, 0:BLOCK_DIM_Y+1, 0:BLOCK_DIM_Z+1)
        ! double precision, shared    :: thetaBC3_sub_block(BLOCK_DIM_X, BLOCK_DIM_Z), thetaBC4_sub_block(BLOCK_DIM_X, BLOCK_DIM_Z)

        ! Temporary variables for coefficient computations
        double precision :: dedx1, dedx2, dedy3, dedy4, dedz5, dedz6    ! Derivative terms
        double precision :: viscous                                     ! Viscous terms
        double precision :: ebc_down, ebc_up, ebc                       ! Boundary terms
        double precision :: eAPI, eAMI, eACI                            ! Diffusion treatment terms in x-direction
        double precision :: eAPJ, eAMJ, eACJ                            ! Diffusion treatment terms in y-direction
        double precision :: eAPK, eAMK, eACK                            ! Diffusion treatment terms in z-direction
        double precision :: eRHS                                        ! From eAPI to eACK
        double precision :: inv_dmx_sub_i, inv_dmx_sub_ip, inv_dmy_sub_j, inv_dmy_sub_jp, inv_dmz_sub_k, inv_dmz_sub_kp
        double precision :: thetaijk, thetaip, thetaim, thetajp, thetajm, thetakp, thetakm
        double precision :: thetaBC3, thetaBC4

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        ti = threadidx%x
        tj = threadidx%y
        tk = threadidx%z

        ! theta_block(ti, tj, tk) = theta_d(i, j, k)

        ! if(ti.eq.1) then
        !     theta_block(ti-1, tj, tk) = theta_d(i-1, j, k)
        ! else if (ti.eq.blockdim%x) then
        !     theta_block(ti+1, tj, tk) = theta_d(i+1, j, k)
        ! endif

        ! if(tj.eq.1) then
        !     theta_block(ti, tj-1, tk) = theta_d(i, j-1, k)
        ! else if (tj.eq.blockdim%y) then
        !     theta_block(ti, tj+1, tk) = theta_d(i, j+1, k)
        ! endif

        ! if(tj.eq.2) then
        !     thetaBC3_sub_block(ti,tk) = thetaBC3_sub_d(i,k)
        !     thetaBC4_sub_block(ti,tk) = thetaBC4_sub_d(i,k)
        ! endif

        ! if(tk.eq.1) then
        !     theta_block(ti, tj, tk-1) = theta_d(i, j, k-1)
        ! else if (tk.eq.blockdim%z) then
        !     theta_block(ti, tj, tk+1) = theta_d(i, j, k+1)
        ! endif

        ! call syncthreads()

        thetaijk = theta_d(i,  j,  k)
        thetaip  = theta_d(i+1,j,  k)
        thetaim  = theta_d(i-1,j,  k)
        thetajp  = theta_d(i,  j+1,k)
        thetajm  = theta_d(i,  j-1,k)
        thetakp  = theta_d(i,  j,  k+1)
        thetakm  = theta_d(i,  j,  k-1)

        thetaBC3 = thetaBC3_sub_d(i,k)
        thetaBC4 = thetaBC4_sub_d(i,k)

        inv_dmx_sub_i  = 1.0d0 / dmx_sub_d(i  )
        inv_dmx_sub_ip = 1.0d0 / dmx_sub_d(i+1)
        inv_dmy_sub_j  = 1.0d0 / dmy_sub_d(j  )
        inv_dmy_sub_jp = 1.0d0 / dmy_sub_d(j+1)
        inv_dmz_sub_k  = 1.0d0 / dmz_sub_d(k  )
        inv_dmz_sub_kp = 1.0d0 / dmz_sub_d(k+1)

        jep = jpbc_index_d(j)
        jem = jmbc_index_d(j)

        ! Diffusion term
        dedx1 = (  thetaijk - thetaim  )*inv_dmx_sub_i
        dedx2 = (  thetaip  - thetaijk )*inv_dmx_sub_ip
        dedy3 = (  thetaijk - thetajm  )*inv_dmy_sub_j
        dedy4 = (  thetajp  - thetaijk )*inv_dmy_sub_jp
        dedz5 = (  thetaijk - thetakm  )*inv_dmz_sub_k
        dedz6 = (  thetakp  - thetaijk )*inv_dmz_sub_kp

        viscous = coefx*(dedx2 - dedx1) + coefy*(dedy4 - dedy3) + coefz*(dedz6 - dedz5)
        
        ! Boundary treatment for the y-direction only
        ebc_down = coefy*inv_dmy_sub_j *thetaBC3
        ebc_up   = coefy*inv_dmy_sub_jp*thetaBC4
        ebc = dble(1.0d0 - jem)*ebc_down + dble(1. - jep)*ebc_up

        ! Diffusion term from incremental notation in next time step: x-direction
        eAPI = -coefx*inv_dmx_sub_ip
        eAMI = -coefx*inv_dmx_sub_i
        eACI =  coefx*( inv_dmx_sub_ip + inv_dmx_sub_i )

        ! Diffusion term from incremental notation in next time step: z-direction
        eAPK = -coefz*inv_dmz_sub_kp
        eAMK = -coefz*inv_dmz_sub_k
        eACK =  coefz*( inv_dmz_sub_kp + inv_dmz_sub_k )

        ! Diffusion term from incremental notation in next time step: y-direction
        eAPJ = -coefy*inv_dmy_sub_jp*dble(jep)
        eAMJ = -coefy*inv_dmy_sub_j *dble(jem)
        eACJ =  coefy*( inv_dmy_sub_jp + inv_dmy_sub_j )

        eRHS = eAPK*thetakp + eACK*thetaijk + eAMK*thetakm      &
            & + eAPJ*thetajp + eACJ*thetaijk + eAMJ*thetajm      &
            & + eAPI*thetaip + eACI*thetaijk + eAMI*thetaim

        ! r.h.s. term 
        rhs_d(i,j,k) = viscous + ebc - eRHS

    end subroutine build_RHS_cuda

    attributes(global) subroutine build_LHSz_cuda(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmz_sub_d, nx_sub, ny_sub, nz_sub, coefz, dt)

        implicit none

        double precision, device, intent(inout) :: ap_ptr(nx_sub-1, ny_sub-1, nz_sub-1), am_ptr(nx_sub-1, ny_sub-1, nz_sub-1)
        double precision, device, intent(inout) :: ac_ptr(nx_sub-1, ny_sub-1, nz_sub-1), ad_ptr(nx_sub-1, ny_sub-1, nz_sub-1)
        double precision, device, intent(in)    :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device, intent(in)    :: dmz_sub_d(0:nz_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefz, dt

        integer :: i,j,k
        integer :: ti,tj,tk
        double precision :: inv_dmz_sub_k, inv_dmz_sub_kp
        double precision, shared    :: rhs_block(BLOCK_DIM_X+1, BLOCK_DIM_Y, BLOCK_DIM_Z)

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        ti = threadidx%x
        tj = threadidx%y
        tk = threadidx%z

        rhs_block(ti,tj,tk) = rhs_d(i,j,k) * dt
        call syncthreads()

        inv_dmz_sub_k  = 1.0d0 / dmz_sub_d(k  )
        inv_dmz_sub_kp = 1.0d0 / dmz_sub_d(k+1)

        ap_ptr(i,j,k) = -coefz*inv_dmz_sub_kp*dt
        am_ptr(i,j,k) = -coefz*inv_dmz_sub_k *dt
        ac_ptr(i,j,k) =  coefz*( inv_dmz_sub_kp + inv_dmz_sub_k )*dt + 1.d0
        ad_ptr(i,j,k) = rhs_block(ti,tj,tk)
        ! ad_ptr(k,i,j) = rhs_d(i,j,k)*dt

    end subroutine build_LHSz_cuda

    attributes(global) subroutine build_LHSy_cuda(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmy_sub_d, jpbc_index_d, jmbc_index_d, nx_sub, ny_sub, nz_sub, coefy, dt)

        implicit none

        double precision, device, intent(inout) :: ap_ptr(nx_sub-1, nz_sub-1, ny_sub-1), am_ptr(nx_sub-1, nz_sub-1, ny_sub-1)
        double precision, device, intent(inout) :: ac_ptr(nx_sub-1, nz_sub-1, ny_sub-1), ad_ptr(nx_sub-1, nz_sub-1, ny_sub-1)
        double precision, device, intent(in)    :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device, intent(in)    :: dmy_sub_d(0:ny_sub)
        integer, device, intent(in)             :: jpbc_index_d(0:ny_sub), jmbc_index_d(0:ny_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefy, dt

        integer :: i,j,k
        integer :: ti,tj,tk
        integer :: jep, jem
        double precision :: inv_dmy_sub_j, inv_dmy_sub_jp
        double precision, shared    :: rhs_block(BLOCK_DIM_X+1, BLOCK_DIM_Y, BLOCK_DIM_Z)

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        ti = threadidx%x
        tj = threadidx%y
        tk = threadidx%z

        rhs_block(ti,tj,tk) = rhs_d(i,j,k)
        call syncthreads()

        inv_dmy_sub_j  = 1.0d0 / dmy_sub_d(j  )
        inv_dmy_sub_jp = 1.0d0 / dmy_sub_d(j+1)

        jep = jpbc_index_d(j)
        jem = jmbc_index_d(j)
        
        ap_ptr(i,k,j) = -coefy*inv_dmy_sub_jp*dble(jep)*dt
        am_ptr(i,k,j) = -coefy*inv_dmy_sub_j *dble(jem)*dt
        ac_ptr(i,k,j) =  coefy*( inv_dmy_sub_jp + inv_dmy_sub_j )*dt + 1.d0
        ad_ptr(i,k,j) = rhs_block(ti,tj,tk)
        ! ad_ptr(j,i,k) = rhs_d(i,j,k)

    end subroutine build_LHSy_cuda

    attributes(global) subroutine build_LHSx_cuda(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmx_sub_d, nx_sub, ny_sub, nz_sub, coefx, dt)

        implicit none

        double precision, device, intent(inout) :: ap_ptr(ny_sub-1, nz_sub-1, nx_sub-1), am_ptr(ny_sub-1, nz_sub-1, nx_sub-1)
        double precision, device, intent(inout) :: ac_ptr(ny_sub-1, nz_sub-1, nx_sub-1), ad_ptr(ny_sub-1, nz_sub-1, nx_sub-1)
        double precision, device, intent(in)    :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device, intent(in)    :: dmx_sub_d(0:nx_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefx, dt

        integer :: i,j,k
        double precision :: inv_dmx_sub_i, inv_dmx_sub_ip

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        inv_dmx_sub_i  = 1.0d0 / dmx_sub_d(i  )
        inv_dmx_sub_ip = 1.0d0 / dmx_sub_d(i+1)

        ap_ptr(j,k,i) = -coefx*inv_dmx_sub_ip*dt
        am_ptr(j,k,i) = -coefx*inv_dmx_sub_i*dt
        ac_ptr(j,k,i) =  coefx*( inv_dmx_sub_ip + inv_dmx_sub_i )*dt + 1.d0
        ad_ptr(j,k,i) = rhs_d(i,j,k)

    end subroutine build_LHSx_cuda

    attributes(global) subroutine transpose_ijk2ijk(ad_ptr, rhs_d, nx_sub, ny_sub, nz_sub)

        implicit none

        double precision, device, intent(in)    :: ad_ptr(nx_sub-1, ny_sub-1, nz_sub-1)
        double precision, device, intent(out)   :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        integer :: i,j,k

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        rhs_d(i,j,k)=ad_ptr(i,j,k)

    end subroutine transpose_ijk2ijk

    attributes(global) subroutine transpose_ikj2ijk(ad_ptr, rhs_d, nx_sub, ny_sub, nz_sub)

        implicit none

        double precision, device, intent(in)    :: ad_ptr(nx_sub-1, nz_sub-1, ny_sub-1)
        double precision, device, intent(out)   :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        integer :: i,j,k

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        rhs_d(i,j,k)=ad_ptr(i,k,j)

    end subroutine transpose_ikj2ijk

    attributes(global) subroutine update_theta_cuda(ad_ptr, theta_d, nx_sub, ny_sub, nz_sub)

        implicit none

        double precision, device, intent(in)    :: ad_ptr(ny_sub-1, nz_sub-1, nx_sub-1)
        double precision, device, intent(inout) :: theta_d(0:nx_sub, 0:ny_sub, 0:nz_sub)            ! r.h.s. array
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        integer :: i,j,k

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        theta_d(i,j,k) = theta_d(i,j,k) + ad_ptr(j,k,i)

    end subroutine update_theta_cuda

end module solve_theta_cuda


