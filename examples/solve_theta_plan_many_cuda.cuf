!======================================================================================================================
!> @file        solve_theta_plan_many.f90
!> @brief       This file contains a solver subroutine for the example problem of PaScaL_TDMA.
!> @details     The target example problem is the three-dimensional time-dependent heat conduction problem 
!>              in a unit cube domain applied with the boundary conditions of vertically constant temperature 
!>              and horizontally periodic boundaries.
!> @author      
!>              - Kiha Kim (k-kiha@yonsei.ac.kr), Department of Computational Science & Engineering, Yonsei University
!>              - Ji-Hoon Kang (jhkang@kisti.re.kr), Korea Institute of Science and Technology Information
!>              - Jung-Il Choi (jic@yonsei.ac.kr), Department of Computational Science & Engineering, Yonsei University
!>
!> @date        June 2019
!> @version     1.0
!> @par         Copyright
!>              Copyright (c) 2019 Kiha Kim and Jung-Il choi, Yonsei University and 
!>              Ji-Hoon Kang, Korea Institute of Science and Technology Information, All rights reserved.
!> @par         License     
!>              This project is release under the terms of the MIT License (see LICENSE in )
!======================================================================================================================
module solve_theta_cuda

contains
    !>
    !>
    !> @brief       An example solver for many tridiagonal systems of equations using PaScaL_TDMA.
    !> @details     This subroutine is for many tridiagonal systems of equations.
    !>              It solves the the three-dimensional time-dependent heat conduction problem using PaScaL_TDMA.
    !>              PaScaL_TDMA plans are created for many tridiagonal systems of equations and
    !>              the many tridiagonal systems are solved plane-by-plane.
    !> @param       theta       Main 3-D variable to be solved
    !>
    subroutine solve_theta_plan_many_cuda(theta)

        use omp_lib
        use mpi
        use global
        use mpi_subdomain
        use mpi_topology
        use PaScaL_TDMA_cuda
        use cudafor
        use nvtx

        implicit none

        double precision, dimension(0:nx_sub, 0:ny_sub, 0:nz_sub), intent(inout) :: theta

        type(ptdma_plan_many_cuda)  :: px_many, pz_many, py_many  ! Plan for many tridiagonal systems of equations

        integer :: i, j, k
        integer :: myrank, ierr, request(12), istat
        integer :: time_step        ! Current time step
        double precision :: t_curr  ! Current simulation time
    
        ! Temporary variables for coefficient computations
        double precision :: Ct_half_over_dx, Ct_half_over_dy, Ct_half_over_dz
        double precision, allocatable, dimension(:,:,:) :: rhs            ! r.h.s. array

        double precision, allocatable, dimension(:, :, :), device   :: theta_d, rhs_d                   ! r.h.s. array
        double precision, allocatable, target, dimension(:), device :: ap_d, am_d, ac_d, ad_d           ! Coefficient of tridiagonal matrix
        double precision, pointer, dimension(:,:,:), device         :: ap_ptr, am_ptr, ac_ptr, ad_ptr   ! Coefficient of tridiagonal matrix

        double precision, device    :: dmz_sub_d(0:nz_sub), dmy_sub_d(0:ny_sub), dmx_sub_d(0:nx_sub)
        double precision, device    :: thetaBC3_sub_d(0:nx_sub, 0:nz_sub), thetaBC4_sub_d(0:nx_sub, 0:nz_sub)
        integer, device             :: jpbc_index_d(0:ny_sub), jmbc_index_d(0:ny_sub)

        ! Block and thread dimension
        integer             :: block_in_x, block_in_y, block_in_z
        type(dim3)          :: blocks, threads, threads_in_pascal

        ! Communication buffer
        double precision, allocatable, dimension(:,:), device   :: sbuf_x0(:,:), sbuf_x1(:,:), sbuf_y0(:,:), sbuf_y1(:,:), sbuf_z0(:,:), sbuf_z1(:,:)
        double precision, allocatable, dimension(:,:), device   :: rbuf_x0(:,:), rbuf_x1(:,:), rbuf_y0(:,:), rbuf_y1(:,:), rbuf_z0(:,:), rbuf_z1(:,:)
        
        call MPI_Comm_rank( MPI_COMM_WORLD, myrank, ierr)
    
        Ct_half_over_dx = 0.5d0*Ct/dx
        Ct_half_over_dy = 0.5d0*Ct/dy
        Ct_half_over_dz = 0.5d0*Ct/dz

        dmx_sub_d = dmx_sub
        dmy_sub_d = dmy_sub
        dmz_sub_d = dmz_sub
        jpbc_index_d = jpbc_index
        jmbc_index_d = jmbc_index
        thetaBC3_sub_d = thetaBC3_sub
        thetaBC4_sub_d = thetaBC4_sub

        ! Calculating r.h.s.
        allocate(   rhs_d(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1))
        allocate( theta_d(0:nx_sub  , 0:ny_sub  , 0:nz_sub  ))

        allocate( sbuf_x0(0:ny_sub,0:nz_sub), sbuf_x1(0:ny_sub,0:nz_sub) ); sbuf_x0 = 0.0d0; sbuf_x1 = 0.0d0
        allocate( sbuf_y0(0:nx_sub,0:nz_sub), sbuf_y1(0:nx_sub,0:nz_sub) ); sbuf_y0 = 0.0d0; sbuf_y1 = 0.0d0
        allocate( sbuf_z0(0:nx_sub,0:ny_sub), sbuf_z1(0:nx_sub,0:ny_sub) ); sbuf_z0 = 0.0d0; sbuf_z1 = 0.0d0

        allocate( rbuf_x0(0:ny_sub,0:nz_sub), rbuf_x1(0:ny_sub,0:nz_sub) ); rbuf_x0 = 0.0d0; rbuf_x1 = 0.0d0
        allocate( rbuf_y0(0:nx_sub,0:nz_sub), rbuf_y1(0:nx_sub,0:nz_sub) ); rbuf_y0 = 0.0d0; rbuf_y1 = 0.0d0
        allocate( rbuf_z0(0:nx_sub,0:ny_sub), rbuf_z1(0:nx_sub,0:ny_sub) ); rbuf_z0 = 0.0d0; rbuf_z1 = 0.0d0

        ! Setting the thread and block
        block_in_x = (nx_sub-1)/thread_in_x
        if(block_in_x.eq.0 .or. mod((nx_sub-1), thread_in_x)) then
            print '(a,i5,a,i5)', '[Error] ny_sub-1 should be a multiple of thread_in_x. thread_in_x = ',thread_in_x,', nx_sub-1 = ',nx_sub-1
            call MPI_Finalize(ierr)
            stop
        endif

        block_in_y = (ny_sub-1)/thread_in_y
        if(block_in_y.eq.0 .or. mod((ny_sub-1), thread_in_y)) then
            print '(a,i5,a,i5)', '[Error] nz_sub-1 should be a multiple of thread_in_y. thread_in_y = ',thread_in_y,', ny_sub-1 = ',ny_sub-1
            call MPI_Finalize(ierr)
            stop
        endif

        block_in_z = (nz_sub-1)/thread_in_z
        if(block_in_z.eq.0 .or. mod((nz_sub-1), thread_in_z)) then
            print '(a,i5,a,i5)', '[Error] nz_sub-1 should be a multiple of thread_in_z. thread_in_z = ',thread_in_z,', nz_sub-1 = ',nz_sub-1
            call MPI_Finalize(ierr)
            stop
        endif

        blocks  = dim3(block_in_x, block_in_y, block_in_z)
        threads = dim3(thread_in_x, thread_in_y, thread_in_z)

        threads_in_pascal = dim3(thread_in_x_pascal, thread_in_y_pascal, 1)
    
        ! Simulation begins
        t_curr = tStart
        dt = dtstart

        ! Create a PaScaL_TDMA plan for the tridiagonal systems.
        call PaScaL_TDMA_plan_many_create_cuda(pz_many, nx_sub-1, nz_sub-1, ny_sub-1, comm_1d_z%myrank, comm_1d_z%nprocs, comm_1d_z%mpi_comm, threads_in_pascal)
        call PaScaL_TDMA_plan_many_create_cuda(py_many, nx_sub-1, ny_sub-1, nz_sub-1, comm_1d_y%myrank, comm_1d_y%nprocs, comm_1d_y%mpi_comm, threads_in_pascal)
        call PaScaL_TDMA_plan_many_create_cuda(px_many, ny_sub-1, nx_sub-1, nz_sub-1, comm_1d_x%myrank, comm_1d_x%nprocs, comm_1d_x%mpi_comm, threads_in_pascal)

        allocate( ap_d((nx_sub-1)*(ny_sub-1)*(nz_sub-1)) )
        allocate( ac_d((nx_sub-1)*(ny_sub-1)*(nz_sub-1)) )
        allocate( am_d((nx_sub-1)*(ny_sub-1)*(nz_sub-1)) )
        allocate( ad_d((nx_sub-1)*(ny_sub-1)*(nz_sub-1)) )

        !$cuf kernel do <<< *,* >>>
        do i = 1, (nx_sub-1)*(ny_sub-1)*(nz_sub-1)
            am_d(i)=0.0d0
            ac_d(i)=0.0d0
            ap_d(i)=0.0d0
            ad_d(i)=0.0d0
        enddo
    
        theta_d = theta

        do time_step = 1, Tmax

            t_curr = t_curr + dt
            if(myrank==0.and.mod(time_step,10)==1) write(*,*) '[Main] Current time step = ', time_step

            call build_RHS_cuda<<<blocks, threads>>>(theta_d, rhs_d, dmx_sub_d, dmy_sub_d, dmz_sub_d, jpbc_index_d, jmbc_index_d, thetaBC3_sub_d, thetaBC4_sub_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dx, Ct_half_over_dy, Ct_half_over_dz)

            ! solve in the z-direction.
            ap_ptr(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1) => ap_d
            ac_ptr(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1) => ac_d
            am_ptr(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1) => am_d
            ad_ptr(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1) => ad_d

            ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
            call build_LHSz_cuda<<<blocks, threads>>>(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmz_sub_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dz, dt)

            !Solve the tridiagonal systems under the defined plan with periodic boundary conditions.
            call PaScaL_TDMA_many_solve_cycle_cuda(pz_many, am_ptr, ac_ptr, ap_ptr, ad_ptr, nx_sub-1, nz_sub-1, ny_sub-1)

            ! Return the solution to the r.h.s. plane-by-plane
            call transpose_ijk2ijk<<<blocks, threads>>>(ad_ptr, rhs_d, nx_sub, ny_sub, nz_sub)

            nullify( ap_ptr, ac_ptr, am_ptr, ad_ptr )
            ! solve in the y-direction.
            ap_ptr(1:nx_sub-1, 1:nz_sub-1, 1:ny_sub-1) => ap_d
            ac_ptr(1:nx_sub-1, 1:nz_sub-1, 1:ny_sub-1) => ac_d
            am_ptr(1:nx_sub-1, 1:nz_sub-1, 1:ny_sub-1) => am_d
            ad_ptr(1:nx_sub-1, 1:nz_sub-1, 1:ny_sub-1) => ad_d

            ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
            call build_LHSy_cuda<<<blocks, threads>>>(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmy_sub_d, jpbc_index_d, jmbc_index_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dy, dt)

            ! Solve the tridiagonal systems under the defined plan.
            call PaScaL_TDMA_many_solve_cuda(py_many, am_ptr, ac_ptr, ap_ptr, ad_ptr, nx_sub-1, ny_sub-1, nz_sub-1)
            
            ! Return the solution to the r.h.s. plane-by-plane.
            call transpose_ikj2ijk<<<blocks, threads>>>(ad_ptr, rhs_d, nx_sub, ny_sub, nz_sub)

            nullify( ap_ptr, ac_ptr, am_ptr, ad_ptr )

            ! solve in the x-direction.
            ap_ptr(1:nz_sub-1, 1:ny_sub-1, 1:nx_sub-1) => ap_d
            ac_ptr(1:nz_sub-1, 1:ny_sub-1, 1:nx_sub-1) => ac_d
            am_ptr(1:nz_sub-1, 1:ny_sub-1, 1:nx_sub-1) => am_d
            ad_ptr(1:nz_sub-1, 1:ny_sub-1, 1:nx_sub-1) => ad_d

            ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
            call build_LHSx_cuda<<<blocks, threads>>>(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmx_sub_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dx, dt)

            ! Solve the tridiagonal systems under the defined plan with periodic boundary conditions.
            call PaScaL_TDMA_many_solve_cycle_cuda(px_many, am_ptr, ac_ptr, ap_ptr, ad_ptr, ny_sub-1, nx_sub-1, nz_sub-1)

            ! Return the solution to theta plane-by-plane.
            call update_theta_cuda<<<blocks, threads>>>(ad_ptr, theta_d, nx_sub, ny_sub, nz_sub)

            nullify( ap_ptr, ac_ptr, am_ptr, ad_ptr )

            ! ! Update ghostcells from the solution.
            call ghostcell_update_cuda(theta_d, sbuf_x0, sbuf_x1, sbuf_y0, sbuf_y1, sbuf_z0, sbuf_z1 &
                                              , rbuf_x0, rbuf_x1, rbuf_y0, rbuf_y1, rbuf_z0, rbuf_z1)

        end do

        theta = theta_d
        ! Destroy the PaScaL_TDMA plan for the tridiagonal systems.
        call PaScaL_TDMA_plan_many_destroy_cuda(pz_many,comm_1d_z%nprocs)
        call PaScaL_TDMA_plan_many_destroy_cuda(px_many,comm_1d_x%nprocs)
        call PaScaL_TDMA_plan_many_destroy_cuda(py_many,comm_1d_y%nprocs)

        deallocate( ap_d, am_d, ac_d, ad_d )
        deallocate( rhs_d, theta_d )

        deallocate( sbuf_x0, sbuf_x1 )
        deallocate( sbuf_y0, sbuf_y1 )
        deallocate( sbuf_z0, sbuf_z1 )
        deallocate( rbuf_x0, rbuf_x1 )
        deallocate( rbuf_y0, rbuf_y1 )
        deallocate( rbuf_z0, rbuf_z1 )

    end subroutine solve_theta_plan_many_cuda

    subroutine ghostcell_update_cuda(Value_sub_d, sbuf_x0, sbuf_x1, sbuf_y0, sbuf_y1, sbuf_z0, sbuf_z1 &
                                                , rbuf_x0, rbuf_x1, rbuf_y0, rbuf_y1, rbuf_z0, rbuf_z1)

        use mpi
        use global
        use mpi_subdomain
        use mpi_topology
        use cudafor

        implicit none

        double precision, device, dimension(0:nx_sub, 0:ny_sub, 0:nz_sub), intent(inout)  :: Value_sub_d
        double precision, allocatable, dimension(:,:), device   :: sbuf_x0(:,:), sbuf_x1(:,:), sbuf_y0(:,:), sbuf_y1(:,:), sbuf_z0(:,:), sbuf_z1(:,:)
        double precision, allocatable, dimension(:,:), device   :: rbuf_x0(:,:), rbuf_x1(:,:), rbuf_y0(:,:), rbuf_y1(:,:), rbuf_z0(:,:), rbuf_z1(:,:)
        integer :: i, j, k
        integer :: ierr, request(4)
        integer :: nprocs, mpiierr
        integer :: istat(MPI_STATUS_SIZE)
        integer(4) :: version
                
        !X-Direction        
        !$cuf kernel do(2) <<< *,* >>>
        do k = 0, nz_sub
        do j = 0, ny_sub
            if(comm_1d_x%west_rank.ne.MPI_PROC_NULL) then
                sbuf_x0(j,k) = Value_sub_d(1       ,j,k)
            endif
            if(comm_1d_x%east_rank.ne.MPI_PROC_NULL) then
                sbuf_x1(j,k) = Value_sub_d(nx_sub-1,j,k)
            endif
        enddo
        enddo

        if( comm_1d_x%nprocs.eq.1 .and. period(0).eqv..true. ) then
            !$cuf kernel do(2) <<< *,* >>>
            do k = 0, nz_sub
            do j = 0, ny_sub
                rbuf_x1(j,k) = sbuf_x0(j,k)
                rbuf_x0(j,k) = sbuf_x1(j,k) 
            enddo
            enddo
        else
            ierr = cudaStreamSynchronize()
            call MPI_Isend(sbuf_x0, (ny_sub+1)*(nz_sub+1), MPI_DOUBLE_PRECISION, comm_1d_x%west_rank, 111, comm_1d_x%mpi_comm, request(1), ierr)
            call MPI_Irecv(rbuf_x1, (ny_sub+1)*(nz_sub+1), MPI_DOUBLE_PRECISION, comm_1d_x%east_rank, 111, comm_1d_x%mpi_comm, request(2), ierr)
            call MPI_Irecv(rbuf_x0, (ny_sub+1)*(nz_sub+1), MPI_DOUBLE_PRECISION, comm_1d_x%west_rank, 222, comm_1d_x%mpi_comm, request(3), ierr)
            call MPI_Isend(sbuf_x1, (ny_sub+1)*(nz_sub+1), MPI_DOUBLE_PRECISION, comm_1d_x%east_rank, 222, comm_1d_x%mpi_comm, request(4), ierr)
            call MPI_Waitall(4, request, MPI_STATUSES_IGNORE, ierr)
        endif

        !$cuf kernel do(2) <<< *,* >>>
        do k = 0, nz_sub
        do j = 0, ny_sub
            if(comm_1d_x%west_rank.ne.MPI_PROC_NULL) then
                Value_sub_d(0     ,j,k) = rbuf_x0(j,k)
            endif
            if(comm_1d_x%east_rank.ne.MPI_PROC_NULL) then
                Value_sub_d(nx_sub,j,k) = rbuf_x1(j,k)
            endif
        enddo
        enddo

        !Y-Direction    
        !$cuf kernel do(2) <<< *,* >>>
        do k = 0, nz_sub
        do i = 0, nx_sub
            if(comm_1d_y%west_rank.ne.MPI_PROC_NULL) then
                sbuf_y0(i,k) = Value_sub_d(i,1       ,k)
            endif
            if(comm_1d_y%east_rank.ne.MPI_PROC_NULL) then
                sbuf_y1(i,k) = Value_sub_d(i,ny_sub-1,k)
            endif
        enddo
        enddo

        if( comm_1d_y%nprocs.eq.1 .and. period(1).eqv..true. ) then 
            !$cuf kernel do(2) <<< *,* >>>
            do k = 0, nz_sub
            do i = 0, nx_sub
                rbuf_y1(i,k) = sbuf_y0(i,k)
                rbuf_y0(i,k) = sbuf_y1(i,k) 
            enddo
            enddo
        else
            ierr = cudaStreamSynchronize()
            call MPI_Isend(sbuf_y0, (nx_sub+1)*(nz_sub+1), MPI_DOUBLE_PRECISION, comm_1d_y%west_rank, 333, comm_1d_y%mpi_comm, request(1), ierr)
            call MPI_Irecv(rbuf_y1, (nx_sub+1)*(nz_sub+1), MPI_DOUBLE_PRECISION, comm_1d_y%east_rank, 333, comm_1d_y%mpi_comm, request(2), ierr)
            call MPI_Irecv(rbuf_y0, (nx_sub+1)*(nz_sub+1), MPI_DOUBLE_PRECISION, comm_1d_y%west_rank, 444, comm_1d_y%mpi_comm, request(3), ierr)
            call MPI_Isend(sbuf_y1, (nx_sub+1)*(nz_sub+1), MPI_DOUBLE_PRECISION, comm_1d_y%east_rank, 444, comm_1d_y%mpi_comm, request(4), ierr)
            call MPI_Waitall(4, request, MPI_STATUSES_IGNORE, ierr)
        endif

        !$cuf kernel do(2) <<< *,* >>>
        do k = 0, nz_sub
        do i = 0, nx_sub
            if(comm_1d_y%west_rank.ne.MPI_PROC_NULL) then
                Value_sub_d(i,0     ,k) = rbuf_y0(i,k)
            endif
            if(comm_1d_y%east_rank.ne.MPI_PROC_NULL) then
                Value_sub_d(i,ny_sub,k) = rbuf_y1(i,k)
            endif
        enddo
        enddo

        !Z-Direction
        !$cuf kernel do(2) <<< *,* >>>
        do j = 0, ny_sub
        do i = 0, nx_sub
            if(comm_1d_z%west_rank.ne.MPI_PROC_NULL) then
                sbuf_z0(i,j) = Value_sub_d(i,j,1       )
            endif
            if(comm_1d_z%east_rank.ne.MPI_PROC_NULL) then
                sbuf_z1(i,j) = Value_sub_d(i,j,nz_sub-1)
            endif
        enddo
        enddo
       
        if( comm_1d_z%nprocs.eq.1 .and. period(2).eqv..true. ) then
            !$cuf kernel do(2) <<< *,* >>>
            do j = 0, ny_sub
            do i = 0, nx_sub
                rbuf_z1(i,j) = sbuf_z0(i,j)
                rbuf_z0(i,j) = sbuf_z1(i,j) 
            enddo
            enddo
        else
            ierr = cudaStreamSynchronize()
            call MPI_Isend(sbuf_z0, (nx_sub+1)*(ny_sub+1), MPI_DOUBLE_PRECISION, comm_1d_z%west_rank, 555, comm_1d_z%mpi_comm, request(9 ), ierr)
            call MPI_Irecv(rbuf_z1, (nx_sub+1)*(ny_sub+1), MPI_DOUBLE_PRECISION, comm_1d_z%east_rank, 555, comm_1d_z%mpi_comm, request(10), ierr)
            call MPI_Irecv(rbuf_z0, (nx_sub+1)*(ny_sub+1), MPI_DOUBLE_PRECISION, comm_1d_z%west_rank, 666, comm_1d_z%mpi_comm, request(11), ierr)
            call MPI_Isend(sbuf_z1, (nx_sub+1)*(ny_sub+1), MPI_DOUBLE_PRECISION, comm_1d_z%east_rank, 666, comm_1d_z%mpi_comm, request(12), ierr)
            call MPI_Waitall(4, request, MPI_STATUSES_IGNORE, ierr)
        endif

        !$cuf kernel do(2) <<< *,* >>>
        do j = 0, ny_sub
        do i = 0, nx_sub
            if(comm_1d_z%west_rank.ne.MPI_PROC_NULL) then
                Value_sub_d(i,j,0     ) = rbuf_z0(i,j)
            endif
            if(comm_1d_z%east_rank.ne.MPI_PROC_NULL) then
                Value_sub_d(i,j,nz_sub) = rbuf_z1(i,j)
            endif
        enddo
        enddo
        
    end subroutine ghostcell_update_cuda

    !>
    !> @brief   Modified Thomas algorithm : Update solution
    !> @param   plan        Plan for many tridiagonal systems of equations
    !> @param   A           Coefficients in lower diagonal elements
    !> @param   C           Coefficients in upper diagonal elements
    !> @param   D           Coefficients in right-hand side terms
    !> @param   n_sys       Number of tridiagonal systems per process
    !> @param   n_row       Number of rows in each process, size of a tridiagonal matrix N divided by nprocs
    !>
    attributes(global) subroutine build_RHS_cuda(theta_d, rhs_d, dmx_sub_d, dmy_sub_d, dmz_sub_d, jpbc_index_d, jmbc_index_d, thetaBC3_sub_d, thetaBC4_sub_d, nx_sub, ny_sub, nz_sub, coefx, coefy, coefz)

        implicit none

        double precision, device, intent(in)    :: theta_d(0:nx_sub, 0:ny_sub, 0:nz_sub)          ! r.h.s. array
        double precision, device, intent(inout) :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device :: dmz_sub_d(0:nz_sub), dmy_sub_d(0:ny_sub), dmx_sub_d(0:nx_sub)
        double precision, device :: thetaBC3_sub_d(0:nx_sub, 0:nz_sub), thetaBC4_sub_d(0:nx_sub, 0:nz_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefx, coefy, coefz
        integer, device :: jpbc_index_d(0:ny_sub), jmbc_index_d(0:ny_sub)

        ! Loop and index variables
        integer :: i,j,k
        integer :: jem, jep

        ! Temporary variables for coefficient computations
        double precision :: dedx1, dedx2, dedy3, dedy4, dedz5, dedz6    ! Derivative terms
        double precision :: viscous                                     ! Viscous terms
        double precision :: ebc_down, ebc_up, ebc                       ! Boundary terms
        double precision :: eAPI, eAMI, eACI                            ! Diffusion treatment terms in x-direction
        double precision :: eAPJ, eAMJ, eACJ                            ! Diffusion treatment terms in y-direction
        double precision :: eAPK, eAMK, eACK                            ! Diffusion treatment terms in z-direction
        double precision :: eRHS                                        ! From eAPI to eACK
        double precision :: inv_dmx_sub_i, inv_dmx_sub_ip, inv_dmy_sub_j, inv_dmy_sub_jp, inv_dmz_sub_k, inv_dmz_sub_kp
        double precision :: thetaijk, thetaip, thetaim, thetajp, thetajm, thetakp, thetakm
        double precision :: thetaBC3, thetaBC4

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        thetaijk = theta_d(i,  j,  k)
        thetaip  = theta_d(i+1,j,  k)
        thetaim  = theta_d(i-1,j,  k)
        thetajp  = theta_d(i,  j+1,k)
        thetajm  = theta_d(i,  j-1,k)
        thetakp  = theta_d(i,  j,  k+1)
        thetakm  = theta_d(i,  j,  k-1)

        thetaBC3 = thetaBC3_sub_d(i,k)
        thetaBC4 = thetaBC4_sub_d(i,k)

        inv_dmx_sub_i  = 1.0d0 / dmx_sub_d(i  )
        inv_dmx_sub_ip = 1.0d0 / dmx_sub_d(i+1)
        inv_dmy_sub_j  = 1.0d0 / dmy_sub_d(j  )
        inv_dmy_sub_jp = 1.0d0 / dmy_sub_d(j+1)
        inv_dmz_sub_k  = 1.0d0 / dmz_sub_d(k  )
        inv_dmz_sub_kp = 1.0d0 / dmz_sub_d(k+1)

        jep = jpbc_index_d(j)
        jem = jmbc_index_d(j)

        ! Diffusion term
        dedx1 = (  thetaijk - thetaim  )*inv_dmx_sub_i
        dedx2 = (  thetaip  - thetaijk )*inv_dmx_sub_ip
        dedy3 = (  thetaijk - thetajm  )*inv_dmy_sub_j
        dedy4 = (  thetajp  - thetaijk )*inv_dmy_sub_jp
        dedz5 = (  thetaijk - thetakm  )*inv_dmz_sub_k
        dedz6 = (  thetakp  - thetaijk )*inv_dmz_sub_kp

        viscous = coefx*(dedx2 - dedx1) + coefy*(dedy4 - dedy3) + coefz*(dedz6 - dedz5)
        
        ! Boundary treatment for the y-direction only
        ebc_down = coefy*inv_dmy_sub_j *thetaBC3
        ebc_up   = coefy*inv_dmy_sub_jp*thetaBC4
        ebc = dble(1-jem)*ebc_down + dble(1-jep)*ebc_up

        ! Diffusion term from incremental notation in next time step: x-direction
        eAPI = -coefx*inv_dmx_sub_ip
        eAMI = -coefx*inv_dmx_sub_i
        eACI =  coefx*( inv_dmx_sub_ip + inv_dmx_sub_i )

        ! Diffusion term from incremental notation in next time step: z-direction
        eAPK = -coefz*inv_dmz_sub_kp
        eAMK = -coefz*inv_dmz_sub_k
        eACK =  coefz*( inv_dmz_sub_kp + inv_dmz_sub_k )

        ! Diffusion term from incremental notation in next time step: y-direction
        eAPJ = -coefy*inv_dmy_sub_jp*dble(jep)
        eAMJ = -coefy*inv_dmy_sub_j *dble(jem)
        eACJ =  coefy*( inv_dmy_sub_jp + inv_dmy_sub_j )

        eRHS  = eAPK*thetakp + eACK*thetaijk + eAMK*thetakm      &
            & + eAPJ*thetajp + eACJ*thetaijk + eAMJ*thetajm      &
            & + eAPI*thetaip + eACI*thetaijk + eAMI*thetaim

        ! r.h.s. term 
        rhs_d(i,j,k) = viscous + ebc - eRHS

    end subroutine build_RHS_cuda

    attributes(global) subroutine build_LHSz_cuda(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmz_sub_d, nx_sub, ny_sub, nz_sub, coefz, dt)

        implicit none

        double precision, device, intent(inout) :: ap_ptr(nx_sub-1, ny_sub-1, nz_sub-1), am_ptr(nx_sub-1, ny_sub-1, nz_sub-1)
        double precision, device, intent(inout) :: ac_ptr(nx_sub-1, ny_sub-1, nz_sub-1), ad_ptr(nx_sub-1, ny_sub-1, nz_sub-1)
        double precision, device, intent(in)    :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device, intent(in)    :: dmz_sub_d(0:nz_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefz, dt

        integer :: i,j,k
        double precision :: inv_dmz_sub_k, inv_dmz_sub_kp

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        inv_dmz_sub_k  = 1.0d0 / dmz_sub_d(k  )
        inv_dmz_sub_kp = 1.0d0 / dmz_sub_d(k+1)

        ap_ptr(i,j,k) = -coefz*inv_dmz_sub_kp*dt
        am_ptr(i,j,k) = -coefz*inv_dmz_sub_k *dt
        ac_ptr(i,j,k) =  coefz*( inv_dmz_sub_kp + inv_dmz_sub_k )*dt + 1.d0
        ad_ptr(i,j,k) = rhs_d(i,j,k) * dt

    end subroutine build_LHSz_cuda

    attributes(global) subroutine build_LHSy_cuda(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmy_sub_d, jpbc_index_d, jmbc_index_d, nx_sub, ny_sub, nz_sub, coefy, dt)

        implicit none

        double precision, device, intent(inout) :: ap_ptr(nx_sub-1, nz_sub-1, ny_sub-1), am_ptr(nx_sub-1, nz_sub-1, ny_sub-1)
        double precision, device, intent(inout) :: ac_ptr(nx_sub-1, nz_sub-1, ny_sub-1), ad_ptr(nx_sub-1, nz_sub-1, ny_sub-1)
        double precision, device, intent(in)    :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device, intent(in)    :: dmy_sub_d(0:ny_sub)
        integer, device, intent(in)             :: jpbc_index_d(0:ny_sub), jmbc_index_d(0:ny_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefy, dt

        integer :: i,j,k
        integer :: jep, jem
        double precision :: inv_dmy_sub_j, inv_dmy_sub_jp

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        inv_dmy_sub_j  = 1.0d0 / dmy_sub_d(j  )
        inv_dmy_sub_jp = 1.0d0 / dmy_sub_d(j+1)

        jep = jpbc_index_d(j)
        jem = jmbc_index_d(j)
        
        ap_ptr(i,k,j) = -coefy*inv_dmy_sub_jp*dble(jep)*dt
        am_ptr(i,k,j) = -coefy*inv_dmy_sub_j *dble(jem)*dt
        ac_ptr(i,k,j) =  coefy*( inv_dmy_sub_jp + inv_dmy_sub_j )*dt + 1.0d0
        ad_ptr(i,k,j) = rhs_d(i,j,k)

    end subroutine build_LHSy_cuda

    attributes(global) subroutine build_LHSx_cuda(ap_ptr, am_ptr, ac_ptr, ad_ptr, rhs_d, dmx_sub_d, nx_sub, ny_sub, nz_sub, coefx, dt)

        implicit none

        double precision, device, intent(inout) :: ap_ptr(ny_sub-1, nz_sub-1, nx_sub-1), am_ptr(ny_sub-1, nz_sub-1, nx_sub-1)
        double precision, device, intent(inout) :: ac_ptr(ny_sub-1, nz_sub-1, nx_sub-1), ad_ptr(ny_sub-1, nz_sub-1, nx_sub-1)
        double precision, device, intent(in)    :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device, intent(in)    :: dmx_sub_d(0:nx_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefx, dt

        integer :: i,j,k
        double precision :: inv_dmx_sub_i, inv_dmx_sub_ip

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        inv_dmx_sub_i  = 1.0d0 / dmx_sub_d(i  )
        inv_dmx_sub_ip = 1.0d0 / dmx_sub_d(i+1)

        ap_ptr(j,k,i) = -coefx*inv_dmx_sub_ip*dt
        am_ptr(j,k,i) = -coefx*inv_dmx_sub_i*dt
        ac_ptr(j,k,i) =  coefx*( inv_dmx_sub_ip + inv_dmx_sub_i )*dt + 1.0d0
        ad_ptr(j,k,i) = rhs_d(i,j,k)

    end subroutine build_LHSx_cuda

    attributes(global) subroutine transpose_ijk2ijk(ad_ptr, rhs_d, nx_sub, ny_sub, nz_sub)

        implicit none

        double precision, device, intent(in )   :: ad_ptr(nx_sub-1, ny_sub-1, nz_sub-1)
        double precision, device, intent(out)   ::  rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        integer :: i,j,k

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        rhs_d(i,j,k)=ad_ptr(i,j,k)

    end subroutine transpose_ijk2ijk

    attributes(global) subroutine transpose_ikj2ijk(ad_ptr, rhs_d, nx_sub, ny_sub, nz_sub)

        implicit none

        double precision, device, intent(in )   :: ad_ptr(nx_sub-1, nz_sub-1, ny_sub-1)
        double precision, device, intent(out)   :: rhs_d (nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        integer :: i,j,k

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        rhs_d(i,j,k)=ad_ptr(i,k,j)

    end subroutine transpose_ikj2ijk

    attributes(global) subroutine update_theta_cuda(ad_ptr, theta_d, nx_sub, ny_sub, nz_sub)

        implicit none

        double precision, device, intent(in   ) :: ad_ptr(ny_sub-1, nz_sub-1, nx_sub-1)
        double precision, device, intent(inout) :: theta_d(0:nx_sub, 0:ny_sub, 0:nz_sub)            ! r.h.s. array
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        integer :: i,j,k

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        theta_d(i,j,k) = theta_d(i,j,k) + ad_ptr(j,k,i)

    end subroutine update_theta_cuda

end module solve_theta_cuda


