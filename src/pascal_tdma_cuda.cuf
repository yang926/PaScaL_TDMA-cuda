!======================================================================================================================
!> @file        pascal_tdma.f90
!> @brief       PaScaL_TDMA - Parallel and Scalable Library for TriDiagonal Matrix Algorithm
!> @details     PaScal_TDMA provides an efficient and scalable computational procedure 
!>              to solve many tridiagonal systems in multi-dimensional partial differential equations. 
!>              The modified Thomas algorithm proposed by Laszlo et al.(2016) and the newly designed communication 
!>              scheme have been used to reduce the communication overhead in solving many tridiagonal systems.
!>              This library is for both single and many tridiagonal systems of equations. 
!>              The main algorithm for a tridiagonal matrix consists of the following five steps: 
!>
!>              (1) Transform the partitioned submatrices in the tridiagonal systems into modified submatrices:
!>                  Each computing core transforms the partitioned submatrices in the tridiagonal systems 
!>                  of equations into modified forms by applying the modified Thomas algorithm.
!>              (2) Construct reduced tridiagonal systems from the modified submatrices:
!>                  The reduced tridiagonal systems are constructed by collecting the first and last rows 
!>                  of the modified submatrices from each core using MPI_Ialltoallw.
!>              (3) Solve the reduced tridiagonal systems:
!>                  The reduced tridiagonal systems constructed in Step 2 are solved by applying the Thomas algorithm.
!>              (4) Distribute the solutions of the reduced tridiagonal systems:
!>                  The solutions of the reduced tridiagonal systems in Step 3 are distributed to each core 
!>                  using MPI_Ialltoallw. This communication is an exact inverse of the communication in Step 2.
!>              (5) Update the other unknowns in the modified tridiagonal systems:
!>                  The remaining unknowns in the modified submatrices in Step 1 are solved in each computing core 
!>                  using the solutions obtained in Step 3 and Step 4.
!>
!>              Step 1 and Step 5 are similar to the method proposed by Laszlo et al.(2016)
!>              which uses parallel cyclic reduction (PCR) algorithm to build and solve the reduced tridiagonal systems.
!>              Instead of using the PCR, we develop an all-to-all communication scheme using the MPI_Ialltoall
!>              function after the modified Thomas algorithm is executed. The number of coefficients for
!>              the reduced tridiagonal systems are greatly reduced, so we can avoid the communication 
!>              bandwidth problem, which is a main bottleneck for all-to-all communications.
!>              Our algorithm is also distinguished from the work of Mattor et al. (1995) which
!>              assembles the undetermined coefficients of the temporary solutions in a single processor 
!>              using MPI_Gather, where load imbalances are serious.
!> 
!> @author      
!>              - Kiha Kim (k-kiha@yonsei.ac.kr), Department of Computational Science & Engineering, Yonsei University
!>              - Ji-Hoon Kang (jhkang@kisti.re.kr), Korea Institute of Science and Technology Information
!>              - Jung-Il Choi (jic@yonsei.ac.kr), Department of Computational Science & Engineering, Yonsei University
!>
!> @date        June 2019
!> @version     1.0
!> @par         Copyright
!>              Copyright (c) 2019 Kiha Kim and Jung-Il choi, Yonsei University and 
!>              Ji-Hoon Kang, Korea Institute of Science and Technology Information, All rights reserved.
!> @par         License     
!>              This project is released under the terms of the MIT License (see LICENSE )
!======================================================================================================================

!>
!> @brief       Module for PaScaL-TDMA library.
!> @details     It contains plans for tridiagonal systems of equations and subroutines for solving them 
!>              using the defined plans. The operation of the library includes the following three phases:
!>              (1) Create a data structure called a plan, which has the information for communication and reduced systems.
!>              (2) Solve the tridiagonal systems of equations executing from Step 1 to Step 5
!>              (3) Destroy the created plan
!>
module PaScaL_TDMA_cuda

    use mpi
    use omp_lib
    use cudafor
    use openacc
    !use nvtx

    implicit none

    !> @brief   Execution plan for many tridiagonal systems of equations.
    !> @details It uses MPI_Ialltoallw function to distribute the modified tridiagonal systems to MPI processes
    !>          and build the reduced tridiagonal systems of equations. Derived datatypes are defined and used 
    !>          to eliminate the cost of data packing and unpacking.
    type, public :: ptdma_plan_many_cuda

        integer :: ptdma_world      !< Single dimensional subcommunicator to assemble data for the reduced TDMA
        integer :: ptdma_nprocs     !< Size of ptdma_world
        integer :: n_sys_rt         !< Number of tridiagonal systems that need to be solved in each process after transpose
        integer :: n_row_rt         !< Number of rows of a reduced tridiagonal systems after transpose

        !> @{ Send buffer related variables for MPI_Ialltoallw
        integer, allocatable, dimension(:) :: ddtype_FS, count_send, displ_send
        !> @}

        !> @{ Recv. buffer related variables MPI_Ialltoallw 
        integer, allocatable, dimension(:) :: ddtype_BS, count_recv, displ_recv
        !> @}

        !> @{ Coefficient arrays after reduction, a: lower, b: diagonal, c: upper, d: rhs.
        !>    The orginal dimension (m:n) is reduced to (m:2)
        double precision, allocatable, dimension(:,:,:) :: A_rd, B_rd, C_rd, D_rd     
        !> @}

        !> @{ Coefficient arrays after transpose of reduced systems, a: lower, b: diagonal, c: upper, d: rhs
        !>    The reduced dimension (m:2) changes to (m/np: 2*np) after transpose.
        double precision, allocatable, dimension(:,:,:) :: A_rt, B_rt, C_rt, D_rt 
        !> @}

        double precision, allocatable, device, dimension(:,:,:) :: a_rd_d, b_rd_d, c_rd_d, d_rd_d
        double precision, allocatable, device, dimension(:,:,:) :: a_rt_d, b_rt_d, c_rt_d, d_rt_d, e_rt_d

        double precision, allocatable, device, dimension(:) :: sendbuf, recvbuf
    
    end type ptdma_plan_many_cuda

    integer, parameter, private :: BLOCK_DIM_X_PASCAL = 32, BLOCK_DIM_Y_PASCAL = 16
    integer, private :: block_in_x_pascal, block_in_y_pascal, block_rt_in_x_pascal, share_size_pascal = kind(0.0d0)*(1+BLOCK_DIM_X_PASCAL)*BLOCK_DIM_Y_PASCAL
    

    public  :: PaScaL_TDMA_plan_many_create_cuda
    public  :: PaScaL_TDMA_plan_many_destroy_cuda
    public  :: PaScaL_TDMA_many_solve_cuda
    public  :: PaScaL_TDMA_many_solve_cycle_cuda 

    contains

    !>
    !> @brief   Create a plan for many tridiagonal systems of equations.
    !> @param   plan        Plan for a single tridiagonal system of equations
    !> @param   n_sys       Number of tridiagonal systems of equations for process
    !> @param   myrank      Rank ID in mpi_world
    !> @param   nprocs      Number of MPI process in mpi_world
    !> @param   mpi_world   Communicator for MPI_Gather and MPI_Scatter of reduced equations
    !>
    subroutine PaScaL_TDMA_plan_many_create_cuda(plan, n_sys, n_row, n_blk, myrank, nprocs, mpi_world)

        implicit none

        type(ptdma_plan_many_cuda), intent(inout)  :: plan
        integer, intent(in)     :: n_row, n_sys
        integer, intent(in)     :: n_blk
        integer, intent(in)     :: myrank, nprocs, mpi_world

        integer :: i, ierr
        integer :: ista, iend                               ! First and last indices of assigned range in many tridiagonal systems of equations 
        integer :: bigsize(3), subsize(3), start(3)         ! Temporary variables of derived data type (DDT)
        integer :: ns_rd, nr_rd                             ! Dimensions of many reduced tridiagonal systems
        integer :: ns_rt, nr_rt                             ! Dimensions of many reduced tridiagonal systems after transpose
        integer, allocatable, dimension(:):: ns_rt_array    ! Array specifying the number of tridiagonal systems for each process after transpose

        ! Specify dimensions for reduced systems.
        ns_rd = n_sys
        nr_rd = 2

        ! Specify dimensions for reduced systems after transpose.
        ! ns_rt         : divide the number of tridiagonal systems of equations per each process  
        ! ns_rt_array   : save the ns_rt in ns_rt_array for defining the DDT
        ! nr_rt         : dimensions of the reduced tridiagonal systems in the solving direction, nr_rd*nprocs
        call para_range(1, ns_rd, nprocs, myrank, ista, iend)
        ns_rt = iend - ista + 1
        allocate(ns_rt_array(0:nprocs-1))
        call MPI_Allgather(ns_rt, 1, mpi_integer, ns_rt_array,1, mpi_integer, mpi_world, ierr)
        nr_rt = nr_rd*nprocs

        ! Assign plan variables and allocate coefficient arrays.
        plan%n_sys_rt = ns_rt
        plan%n_row_rt = nr_rt
        plan%ptdma_world = mpi_world

        allocate( plan%A_rd(ns_rd, n_blk, nr_rd) )
        allocate( plan%B_rd(ns_rd, n_blk, nr_rd) )
        allocate( plan%C_rd(ns_rd, n_blk, nr_rd) )
        allocate( plan%D_rd(ns_rd, n_blk, nr_rd) )
        allocate( plan%A_rt(ns_rt, n_blk, nr_rt) )
        allocate( plan%B_rt(ns_rt, n_blk, nr_rt) )
        allocate( plan%C_rt(ns_rt, n_blk, nr_rt) )
        allocate( plan%D_rt(ns_rt, n_blk, nr_rt) )

        allocate( plan%a_rd_d(ns_rd, n_blk, nr_rd) )
        allocate( plan%b_rd_d(ns_rd, n_blk, nr_rd) )
        allocate( plan%c_rd_d(ns_rd, n_blk, nr_rd) )
        allocate( plan%d_rd_d(ns_rd, n_blk, nr_rd) )
        allocate( plan%a_rt_d(ns_rt, n_blk, nr_rt) )
        allocate( plan%b_rt_d(ns_rt, n_blk, nr_rt) )
        allocate( plan%c_rt_d(ns_rt, n_blk, nr_rt) )
        allocate( plan%d_rt_d(ns_rt, n_blk, nr_rt) )
        allocate( plan%e_rt_d(ns_rt, n_blk, nr_rt) )

        allocate( plan%sendbuf(n_sys*nr_rt/nprocs*n_blk))
        allocate( plan%recvbuf(n_sys*nr_rt/nprocs*n_blk))


        
        ! Building the DDTs.
        allocate(plan%ddtype_Fs(0:nprocs-1), plan%ddtype_Bs(0:nprocs-1))

        do i=0,nprocs-1
            ! DDT for sending coefficients of the reduced tridiagonal systems using MPI_Ialltoallw communication.
            bigsize(1)=ns_rd
            bigsize(2)=n_blk
            bigsize(3)=nr_rd
            subsize(1)=ns_rt_array(i)
            subsize(2)=n_blk
            subsize(3)=nr_rd
            start(1)=sum(ns_rt_array(0:i)) - ns_rt_array(i)
            start(2)=0
            start(3)=0
            call MPI_Type_create_subarray(  3, bigsize, subsize, start,                     &
                                            MPI_ORDER_FORTRAN, MPI_DOUBLE_PRECISION,        &
                                            plan%ddtype_Fs(i), ierr )
            call MPI_Type_commit(plan%ddtype_Fs(i), ierr)
            ! DDT for receiving coefficients for the transposed systems of reduction using MPI_Ialltoallw communication.
            bigsize(1)=ns_rt
            bigsize(2)=n_blk
            bigsize(3)=nr_rt
            subsize(1)=ns_rt
            subsize(2)=n_blk
            subsize(3)=nr_rd
            start(1)=0
            start(2)=0
            start(3)=nr_rd*i
            call MPI_Type_create_subarray(  3, bigsize, subsize, start,                     &
                                            MPI_ORDER_FORTRAN, MPI_DOUBLE_PRECISION,        &
                                            plan%ddtype_Bs(i), ierr )
            call MPI_Type_commit(plan%ddtype_Bs(i), ierr)
        enddo

        ! Buffer counts and displacements for MPI_Ialltoallw.
        ! All buffer counts are 1 and displacements are 0 due to the defined DDT.
        allocate(plan%count_send(0:nprocs-1), plan%displ_send(0:nprocs-1))
        allocate(plan%count_recv(0:nprocs-1), plan%displ_recv(0:nprocs-1))
        plan%count_send=1; plan%displ_send=0
        plan%count_recv=1; plan%displ_recv=0
        plan%ptdma_nprocs = nprocs

        ! Deallocate local array.
        if(allocated(ns_rt_array)) deallocate(ns_rt_array)

    end subroutine PaScaL_TDMA_plan_many_create_cuda

    subroutine PaScaL_TDMA_plan_many_destroy_cuda(plan,nprocs)
        implicit none

        type(ptdma_plan_many_cuda), intent(inout)  :: plan
        integer :: i,nprocs,ierr

        do i=0,nprocs-1
            call MPI_TYPE_FREE(plan%ddtype_Fs(i), ierr)
            call MPI_TYPE_FREE(plan%ddtype_Bs(i), ierr)
        enddo

        deallocate(plan%ddtype_Fs,  plan%ddtype_Bs)
        deallocate(plan%count_send, plan%displ_send)
        deallocate(plan%count_recv, plan%displ_recv)
        deallocate(plan%A_rd, plan%B_rd, plan%C_rd, plan%D_rd)
        deallocate(plan%A_rt, plan%B_rt, plan%C_rt, plan%D_rt)

        deallocate(plan%a_rd_d, plan%b_rd_d, plan%c_rd_d, plan%d_rd_d)
        deallocate(plan%a_rt_d, plan%b_rt_d, plan%c_rt_d, plan%d_rt_d, plan%e_rt_d)

        deallocate( plan%sendbuf, plan%recvbuf )

    end subroutine PaScaL_TDMA_plan_many_destroy_cuda

    subroutine PaScaL_TDMA_many_solve_cuda(plan, a_d, b_d, c_d, d_d, n_sys, n_row, n_blk, time_1, time_2, time_3, time_4, time_5, time_6)

        implicit none

        type(ptdma_plan_many_cuda), intent(inout)   :: plan
        integer, value, intent(in)                  :: n_sys, n_row, n_blk
        double precision, device, intent(inout)     :: a_d(n_sys,n_blk,n_row), b_d(n_sys,n_blk,n_row)
        double precision, device, intent(inout)     :: c_d(n_sys,n_blk,n_row), d_d(n_sys,n_blk,n_row)

        ! Temporary variables for computation and parameters for MPI functions.
        integer :: request(4),ierr
        integer :: n_row_rt, n_sys_rt
        type(dim3) :: blocks, threads

        integer :: i,j,k
        integer :: myrank

        ! For wall clock check
        double precision :: time_1, time_2, time_3, time_4, time_5, time_6
        double precision :: time_1a, time_1b, time_2a, time_2b, time_3a, time_3b
        double precision :: time_4a, time_4b, time_5a, time_5b, time_6a, time_6b

        !call nvtxStartRange("PASCAL-TDMA 1")
        time_1a=mpi_wtime()

        n_row_rt = plan%n_row_rt
        n_sys_rt = plan%n_sys_rt
        

        ! Setting the thread and block
        block_in_x_pascal = n_sys/BLOCK_DIM_X_PASCAL
        if(block_in_x_pascal.eq.0 .or. (mod(n_sys, BLOCK_DIM_X_PASCAL).ne.0)) then
            print '(a,i5,a,i5)', '[Error] n_sys should be a multiple of BLOCK_DIM_X_PASCAL. BLOCK_DIM_X_PASCAL = ',BLOCK_DIM_X_PASCAL,', n_sys = ',n_sys
            call MPI_Finalize(ierr)
            stop
        endif

        block_rt_in_x_pascal = n_sys_rt/BLOCK_DIM_X_PASCAL
        if(block_rt_in_x_pascal.eq.0 .or. (mod(n_sys_rt, BLOCK_DIM_X_PASCAL).ne.0)) then
            print '(a,i5,a,i5)', '[Error] n_sys_rt should be a multiple of BLOCK_DIM_X_PASCAL. BLOCK_DIM_X_PASCAL = ',BLOCK_DIM_X_PASCAL,', n_sys_rt = ',n_sys_rt
            call MPI_Finalize(ierr)
            stop
        endif

        block_in_y_pascal = n_blk/BLOCK_DIM_Y_PASCAL
        if(block_in_y_pascal.eq.0 .or. (mod(n_blk, BLOCK_DIM_Y_PASCAL).ne.0)) then
            print '(a,i5,a,i5)', '[Error] n_blk should be a multiple of BLOCK_DIM_Y_PASCAL. BLOCK_DIM_Y_PASCAL = ',BLOCK_DIM_Y_PASCAL,', n_blk = ',n_blk
            call MPI_Finalize(ierr)
            stop
        endif

 
        threads = dim3(BLOCK_DIM_X_PASCAL, BLOCK_DIM_Y_PASCAL, 1)
        blocks  = dim3(block_in_x_pascal, block_in_y_pascal, 1)

        time_1b=mpi_wtime()
        time_1=time_1b-time_1a
        !call nvtxEndRange
        !call nvtxStartRange("PASCAL-TDMA 2")
        time_2a=mpi_wtime()

        ! The modified Thomas algorithm
        call PaScaL_TDMA_many_modified_Thomas_cuda<<<blocks, threads, 9*share_size_pascal>>>(a_d, b_d, c_d, d_d, plan%a_rd_d, plan%b_rd_d, plan%c_rd_d, plan%d_rd_d, n_sys, n_row, n_blk)
        ! Transpose the reduced systems of equations for TDMA using MPI_Ialltoallw and DDTs.
        ierr = cudaDeviceSynchronize()
        time_2b=mpi_wtime()
        time_2=time_2b-time_2a
        !call nvtxEndRange
        
        !call nvtxStartRange("PASCAL-TDMA 3")
        time_3a=mpi_wtime()
        if(plan%ptdma_nprocs.eq.1) then
            plan%a_rt_d = plan%a_rd_d
            plan%b_rt_d = plan%b_rd_d
            plan%c_rt_d = plan%c_rd_d
            plan%d_rt_d = plan%d_rd_d
        else
            !ierr = cudaDeviceSynchronize()
            call alltoall_kforward(plan%a_rd_d, plan%a_rt_d, n_sys, n_blk, n_row_rt/plan%ptdma_nprocs, plan%ptdma_world, plan%ptdma_nprocs, plan%sendbuf, plan%recvbuf)
            call alltoall_kforward(plan%b_rd_d, plan%b_rt_d, n_sys, n_blk, n_row_rt/plan%ptdma_nprocs, plan%ptdma_world, plan%ptdma_nprocs, plan%sendbuf, plan%recvbuf)
            call alltoall_kforward(plan%c_rd_d, plan%c_rt_d, n_sys, n_blk, n_row_rt/plan%ptdma_nprocs, plan%ptdma_world, plan%ptdma_nprocs, plan%sendbuf, plan%recvbuf)
            call alltoall_kforward(plan%d_rd_d, plan%d_rt_d, n_sys, n_blk, n_row_rt/plan%ptdma_nprocs, plan%ptdma_world, plan%ptdma_nprocs, plan%sendbuf, plan%recvbuf)
           ! ierr = cudaDeviceSynchronize()
        endif
        time_3b=mpi_wtime()
        time_3=time_3b-time_3a
        !call nvtxEndRange

        !call nvtxStartRange("PASCAL-TDMA 4")
        time_4a=mpi_wtime()
        ! Solve the reduced tridiagonal systems of equations using Thomas algorithm.
        threads = dim3(BLOCK_DIM_X_PASCAL, BLOCK_DIM_Y_PASCAL, 1)
        blocks  = dim3(block_rt_in_x_pascal, block_in_y_pascal, 1)

        call tdma_many_cuda<<<blocks, threads, 6*share_size_pascal>>>(plan%a_rt_d,plan%b_rt_d,plan%c_rt_d,plan%d_rt_d, n_sys_rt, n_row_rt, n_blk)
        ! Transpose the obtained solutions to original reduced forms using MPI_Ialltoallw and DDTs.
        ierr = cudaDeviceSynchronize()
        time_4b=mpi_wtime()
        time_4=time_4b-time_4a
        !call nvtxEndRange
        !call nvtxStartRange("PASCAL-TDMA 5")
        time_5a=mpi_wtime()
        if(plan%ptdma_nprocs.eq.1) then
            plan%d_rd_d = plan%d_rt_d
        else
          ! ierr = cudaDeviceSynchronize()
           call alltoall_kbackward(plan%d_rd_d, plan%d_rt_d, n_sys, n_blk, n_row_rt/plan%ptdma_nprocs, plan%ptdma_world, plan%ptdma_nprocs, plan%sendbuf, plan%recvbuf)
          ! ierr = cudaDeviceSynchronize()
        endif
        time_5b=mpi_wtime()
        time_5=time_5b-time_5a
        !call nvtxEndRange
        !call nvtxStartRange("PASCAL-TDMA 6")
        time_6a=mpi_wtime()

        ! Update solutions of the modified tridiagonal system with the solutions of the reduced tridiagonal system.
        threads = dim3(BLOCK_DIM_X_PASCAL, BLOCK_DIM_Y_PASCAL, 1)
        blocks  = dim3(block_in_x_pascal, block_in_y_pascal, 1)
        call PaScaL_TDMA_many_update_solution_cuda<<<blocks, threads, 2*share_size_pascal>>>(a_d, c_d, d_d, plan%d_rd_d, n_sys, n_row, n_blk)
        ierr = cudaDeviceSynchronize()
        time_6b=mpi_wtime()
        time_6=time_6b-time_6a
        !call nvtxEndRange
    end subroutine PaScaL_TDMA_many_solve_cuda

    subroutine PaScaL_TDMA_many_solve_cycle_cuda(plan, a_d, b_d, c_d, d_d, n_sys, n_row, n_blk)

        implicit none

        type(ptdma_plan_many_cuda), intent(inout)   :: plan
        integer, value, intent(in)                  :: n_sys, n_row, n_blk
        double precision, device, intent(inout)     :: a_d(n_sys,n_blk,n_row), b_d(n_sys,n_blk,n_row)
        double precision, device, intent(inout)     :: c_d(n_sys,n_blk,n_row), d_d(n_sys,n_blk,n_row)

        ! Temporary variables for computation and parameters for MPI functions.
        integer :: request(4), ierr
        integer :: n_row_rt, n_sys_rt
        type(dim3) :: blocks, threads

        n_row_rt = plan%n_row_rt
        n_sys_rt = plan%n_sys_rt

        ! Setting the thread and block
        block_in_x_pascal = n_sys/BLOCK_DIM_X_PASCAL
        if(block_in_x_pascal.eq.0 .or. (mod(n_sys, BLOCK_DIM_X_PASCAL).ne.0)) then
            print '(a,i5,a,i5)', '[Error] n_sys should be a multiple of BLOCK_DIM_X_PASCAL. BLOCK_DIM_X_PASCAL = ',BLOCK_DIM_X_PASCAL,', n_sys = ',n_sys
            call MPI_Finalize(ierr)
            stop
        endif

        block_rt_in_x_pascal = n_sys_rt/BLOCK_DIM_X_PASCAL
        if(block_rt_in_x_pascal.eq.0 .or. (mod(n_sys_rt, BLOCK_DIM_X_PASCAL).ne.0)) then
            print '(a,i5,a,i5)', '[Error] n_sys_rt should be a multiple of BLOCK_DIM_X_PASCAL. BLOCK_DIM_X_PASCAL = ',BLOCK_DIM_X_PASCAL,', n_sys_rt = ',n_sys_rt
            call MPI_Finalize(ierr)
            stop
        endif

        block_in_y_pascal = n_blk/BLOCK_DIM_Y_PASCAL
        if(block_in_y_pascal.eq.0 .or. (mod(n_blk, BLOCK_DIM_Y_PASCAL).ne.0)) then
            print '(a,i5,a,i5)', '[Error] n_blk should be a multiple of BLOCK_DIM_Y_PASCAL. BLOCK_DIM_Y_PASCAL = ',BLOCK_DIM_Y_PASCAL,', n_blk = ',n_blk
            call MPI_Finalize(ierr)
            stop
        endif

        threads = dim3(BLOCK_DIM_X_PASCAL, BLOCK_DIM_Y_PASCAL, 1)
        blocks  = dim3(block_in_x_pascal, block_in_y_pascal, 1)

        ! The modified Thomas algorithm
        !call PaScaL_TDMA_many_modified_Thomas_nonbank_cuda<<<blocks, threads>>>(a_d, b_d, c_d, d_d, plan%a_rd_d, plan%b_rd_d, plan%c_rd_d, plan%d_rd_d, n_row)
        call PaScaL_TDMA_many_modified_Thomas_cuda<<<blocks, threads, 9*share_size_pascal>>>(a_d, b_d, c_d, d_d, plan%a_rd_d, plan%b_rd_d, plan%c_rd_d, plan%d_rd_d, n_sys, n_row, n_blk)
        ! Transpose the reduced systems of equations for TDMA using MPI_Ialltoallw and DDTs.
        if(plan%ptdma_nprocs.eq.1) then
            plan%a_rt_d = plan%a_rd_d
            plan%b_rt_d = plan%b_rd_d
            plan%c_rt_d = plan%c_rd_d
            plan%d_rt_d = plan%d_rd_d
        else
           ! ierr = cudaDeviceSynchronize()
            call alltoall_kforward(plan%a_rd_d, plan%a_rt_d, n_sys, n_blk, n_row_rt/plan%ptdma_nprocs, plan%ptdma_world, plan%ptdma_nprocs, plan%sendbuf, plan%recvbuf)
            call alltoall_kforward(plan%b_rd_d, plan%b_rt_d, n_sys, n_blk, n_row_rt/plan%ptdma_nprocs, plan%ptdma_world, plan%ptdma_nprocs, plan%sendbuf, plan%recvbuf)
            call alltoall_kforward(plan%c_rd_d, plan%c_rt_d, n_sys, n_blk, n_row_rt/plan%ptdma_nprocs, plan%ptdma_world, plan%ptdma_nprocs, plan%sendbuf, plan%recvbuf)
            call alltoall_kforward(plan%d_rd_d, plan%d_rt_d, n_sys, n_blk, n_row_rt/plan%ptdma_nprocs, plan%ptdma_world, plan%ptdma_nprocs, plan%sendbuf, plan%recvbuf)
           ! ierr = cudaDeviceSynchronize()
        endif

        threads = dim3(BLOCK_DIM_X_PASCAL, BLOCK_DIM_Y_PASCAL, 1)
        blocks  = dim3(block_rt_in_x_pascal, block_in_y_pascal, 1)

        ! Solve the reduced tridiagonal systems of equations using Thomas algorithm.
        call tdma_cycl_many_nonbank_cuda<<<blocks, threads>>>(plan%a_rt_d,plan%b_rt_d,plan%c_rt_d,plan%d_rt_d,plan%e_rt_d, n_row_rt)
        !call tdma_cycl_many_cuda<<<blocks, threads, 8*share_size_pascal>>>(plan%a_rt_d,plan%b_rt_d,plan%c_rt_d,plan%d_rt_d,plan%e_rt_d, n_sys_rt, n_row_rt, n_blk)
        ! Transpose the obtained solutions to original reduced forms using MPI_Ialltoallw and DDTs.
        if(plan%ptdma_nprocs.eq.1) then
            plan%d_rd_d = plan%d_rt_d
        else

           ! ierr = cudaDeviceSynchronize()
            call alltoall_kbackward(plan%d_rd_d, plan%d_rt_d, n_sys, n_blk, n_row_rt/plan%ptdma_nprocs, plan%ptdma_world, plan%ptdma_nprocs, plan%sendbuf, plan%recvbuf)
           ! ierr = cudaDeviceSynchronize()
        endif

        threads = dim3(BLOCK_DIM_X_PASCAL, BLOCK_DIM_Y_PASCAL, 1)
        blocks  = dim3(block_in_x_pascal, block_in_y_pascal, 1)
        ! Update solutions of the modified tridiagonal system with the solutions of the reduced tridiagonal system.
        call PaScaL_TDMA_many_update_solution_nonbank_cuda<<<blocks, threads>>>(a_d, c_d, d_d, plan%d_rd_d, n_row)
        !call PaScaL_TDMA_many_update_solution_cuda<<<blocks, threads, 2*share_size_pascal>>>(a_d, c_d, d_d, plan%d_rd_d, n_sys, n_row, n_blk)

    end subroutine PaScaL_TDMA_many_solve_cycle_cuda

    attributes(global) subroutine PaScaL_TDMA_many_modified_Thomas_cuda(a, b, c, d, a_rd, b_rd, c_rd, d_rd, n_sys, n_row, n_blk)

        implicit none

        integer, value, intent(in)      :: n_sys, n_row, n_blk
        double precision, device, intent(inout) :: a(n_sys,n_blk,n_row), c(n_sys,n_blk,n_row), d(n_sys,n_blk,n_row)
        double precision, device, intent(in)    :: b(n_sys,n_blk,n_row)
        double precision, device, intent(inout)   :: a_rd(n_sys, n_blk, 2), b_rd(n_sys, n_blk, 2), c_rd(n_sys, n_blk, 2), d_rd(n_sys, n_blk, 2)

        ! Temporary variables for computation
        integer :: i, j, k
        integer :: ti, tj, tk
        double precision :: r

        ! Block shared memory
        double precision, shared :: a1(blockdim%x+1, blockdim%y), a0(blockdim%x+1, blockdim%y)
        double precision, shared :: b1(blockdim%x+1, blockdim%y), b0(blockdim%x+1, blockdim%y)
        double precision, shared :: c1(blockdim%x+1, blockdim%y), c0(blockdim%x+1, blockdim%y)
        double precision, shared :: d1(blockdim%x+1, blockdim%y), d0(blockdim%x+1, blockdim%y)
        double precision, shared :: r0(blockdim%x+1, blockdim%y)

        j = (blockidx%x-1) * blockdim%x + threadidx%x
        k = (blockidx%y-1) * blockdim%y + threadidx%y

        tj = threadidx%x
        tk = threadidx%y

        ! The modified Thomas algorithm : elimination of lower diagonal elements. 
        ! First index indicates a row number in a partitioned tridiagonal system .
        ! Second & Third index indicate a number of independent many tridiagonal systems for parallezation.
        ! The modified Thomas algorithm : elimination of lower diagonal elements. 

        a0(tj,tk) = a(j,k,1)
        b0(tj,tk) = b(j,k,1)
        c0(tj,tk) = c(j,k,1)
        d0(tj,tk) = d(j,k,1)

        a0(tj,tk) = a0(tj,tk) / b0(tj,tk)
        c0(tj,tk) = c0(tj,tk) / b0(tj,tk)
        d0(tj,tk) = d0(tj,tk) / b0(tj,tk)

        a(j,k,1) = a0(tj,tk)
        c(j,k,1) = c0(tj,tk)
        d(j,k,1) = d0(tj,tk)

        a1(tj,tk) = a(j,k,2)
        b1(tj,tk) = b(j,k,2)
        c1(tj,tk) = c(j,k,2)
        d1(tj,tk) = d(j,k,2)

        a1(tj,tk) = a1(tj,tk) / b1(tj,tk)
        c1(tj,tk) = c1(tj,tk) / b1(tj,tk)
        d1(tj,tk) = d1(tj,tk) / b1(tj,tk)

        a(j,k,2) = a1(tj,tk)
        c(j,k,2) = c1(tj,tk)
        d(j,k,2) = d1(tj,tk)


        do i=3, n_row

            a0(tj,tk) = a1(tj,tk)
            c0(tj,tk) = c1(tj,tk)
            d0(tj,tk) = d1(tj,tk)

            a1(tj,tk) = a(j,k,i)
            b1(tj,tk) = b(j,k,i)
            c1(tj,tk) = c(j,k,i)
            d1(tj,tk) = d(j,k,i)

            r0(tj,tk) =  1.0d0/(b1(tj,tk)-a1(tj,tk)*c0(tj,tk))
            d1(tj,tk) =  r0(tj,tk)*(d1(tj,tk)-a1(tj,tk)*d0(tj,tk))
            c1(tj,tk) =  r0(tj,tk)*c1(tj,tk)
            a1(tj,tk) = -r0(tj,tk)*a1(tj,tk)*a0(tj,tk)

            a(j,k,i) = a1(tj,tk)
            c(j,k,i) = c1(tj,tk)
            d(j,k,i) = d1(tj,tk)

        enddo

        ! Construct many reduced tridiagonal systems per each rank. Each process has two rows of reduced systems.
        a_rd(j,k,2) = a1(tj,tk)
        b_rd(j,k,2) = 1.0d0
        c_rd(j,k,2) = c1(tj,tk)
        d_rd(j,k,2) = d1(tj,tk)

        a1(tj,tk) = a0(tj,tk)
        c1(tj,tk) = c0(tj,tk)
        d1(tj,tk) = d0(tj,tk)

        ! The modified Thomas algorithm : elimination of upper diagonal elements.
        do i=n_row-2, 2, -1

            a0(tj,tk) = a(j,k,i)
            c0(tj,tk) = c(j,k,i)
            d0(tj,tk) = d(j,k,i)

            d0(tj,tk) = d0(tj,tk)-c0(tj,tk)*d1(tj,tk)
            a0(tj,tk) = a0(tj,tk)-c0(tj,tk)*a1(tj,tk)
            c0(tj,tk) =-c0(tj,tk)*c1(tj,tk)

            a1(tj,tk) = a0(tj,tk)
            c1(tj,tk) = c0(tj,tk)
            d1(tj,tk) = d0(tj,tk)

            a(j,k,i) = a0(tj,tk)
            c(j,k,i) = c0(tj,tk)
            d(j,k,i) = d0(tj,tk)
        enddo

        a0(tj,tk) = a(j,k,1)
        c0(tj,tk) = c(j,k,1)
        d0(tj,tk) = d(j,k,1)

        r0(tj,tk) = 1.0d0/(1.0d0-a1(tj,tk)*c0(tj,tk))
        d0(tj,tk) =  r0(tj,tk)*(d0(tj,tk)-c0(tj,tk)*d1(tj,tk))
        a0(tj,tk) =  r0(tj,tk)*a0(tj,tk)
        c0(tj,tk) = -r0(tj,tk)*c0(tj,tk)*c1(tj,tk)

        d(j,k,1) = d0(tj,tk)
        a(j,k,1) = a0(tj,tk)
        c(j,k,1) = c0(tj,tk)

        ! Construct many reduced tridiagonal systems per each rank. Each process has two rows of reduced systems.
        a_rd(j,k,1) = a0(tj,tk)
        b_rd(j,k,1) = 1.0d0
        c_rd(j,k,1) = c0(tj,tk)
        d_rd(j,k,1) = d0(tj,tk)
        

    end subroutine PaScaL_TDMA_many_modified_Thomas_cuda

    attributes(global) subroutine PaScaL_TDMA_many_update_solution_cuda(a, c, d, d_rd, n_sys, n_row, n_blk)

        implicit none

        integer, value, intent(in)      :: n_sys, n_row, n_blk
        double precision, device, intent(in)    :: a(n_sys, n_blk, n_row), c(n_sys, n_blk, n_row), d_rd(n_sys, n_blk, 2)
        double precision, device, intent(inout) :: d(n_sys, n_blk, n_row)

        ! Temporary variables for computation
        integer :: i, j, k
        integer :: tj, tk

        ! Block shared memory
        double precision, shared :: ds(blockdim%x+1, blockdim%y), de(blockdim%x+1, blockdim%y)

        j = (blockidx%x-1) * blockdim%x + threadidx%x
        k = (blockidx%y-1) * blockdim%y + threadidx%y

        tj = threadidx%x
        tk = threadidx%y

        ds(tj, tk) = d_rd(j,k,1)
        de(tj, tk) = d_rd(j,k,2)
        call syncthreads()

        ! Update solutions of the modified tridiagonal system with the solutions of the reduced tridiagonal system.
        d(j,k,1)     = ds(tj, tk)
        d(j,k,n_row) = de(tj, tk)

        do i=2,n_row-1
            d(j,k,i) = d(j,k,i)-a(j,k,i)*ds(tj, tk)-c(j,k,i)*de(tj, tk)
        enddo

    end subroutine PaScaL_TDMA_many_update_solution_cuda

    attributes(global) subroutine tdma_many_cuda(a, b, c, d, n_sys_rt, n_row_rt, n_blk)

        implicit none

        integer, value, intent(in)              :: n_sys_rt, n_row_rt, n_blk
        double precision, device, intent(in)    :: a(n_sys_rt, n_blk, n_row_rt), b(n_sys_rt, n_blk, n_row_rt)
        double precision, device, intent(inout) :: c(n_sys_rt, n_blk, n_row_rt), d(n_sys_rt, n_blk, n_row_rt)
        
        integer :: i, j, k
        integer :: tj, tk
        double precision :: r

        double precision, shared :: a1(blockdim%x+1, blockdim%y)
        double precision, shared :: b1(blockdim%x+1, blockdim%y)
        double precision, shared :: c0(blockdim%x+1, blockdim%y), c1(blockdim%x+1, blockdim%y)
        double precision, shared :: d0(blockdim%x+1, blockdim%y), d1(blockdim%x+1, blockdim%y)

        j = (blockidx%x-1) * blockdim%x + threadidx%x
        k = (blockidx%y-1) * blockdim%y + threadidx%y

        tj = threadidx%x
        tk = threadidx%y

        b1(tj, tk) = b(j, k, 1)
        c1(tj, tk) = c(j, k, 1)
        d1(tj, tk) = d(j, k, 1)

        d1(tj, tk) = d1(tj, tk) / b1(tj, tk)
        c1(tj, tk) = c1(tj, tk) / b1(tj, tk)
 
        d(j,k,1)=d1(tj, tk)
        c(j,k,1)=c1(tj, tk)

        do i=2,n_row_rt
            c0(tj, tk) = c1(tj, tk)
            d0(tj, tk) = d1(tj, tk)

            a1(tj, tk) = a(j, k, i)
            b1(tj, tk) = b(j, k, i)
            c1(tj, tk) = c(j, k, i)
            d1(tj, tk) = d(j, k, i)

            r=1.0d0/(b1(tj,tk)-a1(tj,tk)*c0(tj,tk))
            d1(tj,tk)=r*(d1(tj,tk)-a1(tj,tk)*d0(tj,tk))
            c1(tj,tk)=r*c1(tj,tk)

            d(j,k,i)=d1(tj,tk)
            c(j,k,i)=c1(tj,tk)

        enddo

        do i=n_row_rt-1,1,-1
            c0(tj,tk) = c(j,k,i)
            d0(tj,tk) = d(j,k,i)
            d0(tj,tk) = d0(tj,tk)-c0(tj,tk)*d1(tj,tk)
            d1(tj,tk) = d0(tj,tk)
            d(j,k,i)=d0(tj,tk)
        enddo

    end subroutine tdma_many_cuda

    attributes(global) subroutine tdma_cycl_many_cuda(a, b, c, d, e, n_sys_rt, n_row_rt, n_blk)

        implicit none

        integer, value, intent(in)      :: n_sys_rt, n_row_rt, n_blk
        double precision, device, intent(in)    :: a(:,:,:), b(:,:,:)
        double precision, device, intent(inout) :: c(:,:,:), d(:,:,:), e(:,:,:)

        integer :: i, j, k
        integer :: tj, tk
        ! double precision :: e(1:n1)
        double precision :: r

        double precision, shared :: a1(blockdim%x+1, blockdim%y)
        double precision, shared :: b1(blockdim%x+1, blockdim%y)
        double precision, shared :: c0(blockdim%x+1, blockdim%y), c1(blockdim%x+1, blockdim%y)
        double precision, shared :: d0(blockdim%x+1, blockdim%y), d1(blockdim%x+1, blockdim%y)
        double precision, shared :: e0(blockdim%x+1, blockdim%y), e1(blockdim%x+1, blockdim%y)

        j = (blockidx%x-1) * blockdim%x + threadidx%x
        k = (blockidx%y-1) * blockdim%y + threadidx%y

        tj = threadidx%x
        tk = threadidx%y

        do i=1,n_row_rt
            e(j,k,i)  = 0.0d0
        enddo

        e(j,k,2)  = -a(j,k,2)
        e(j,k,n_row_rt) = -c(j,k,n_row_rt)

        d1(tj,tk) = d(j,k,2)
        b1(tj,tk) = b(j,k,2)
        c1(tj,tk) = c(j,k,2)
        e1(tj,tk) = e(j,k,2)

        d1(tj,tk) = d1(tj,tk) / b1(tj,tk)
        c1(tj,tk) = c1(tj,tk) / b1(tj,tk)
        e1(tj,tk) = e1(tj,tk) / b1(tj,tk)

        d(j,k,2)  = d1(tj,tk)
        c(j,k,2)  = c1(tj,tk)
        e(j,k,2)  = e1(tj,tk)

        do i=3,n_row_rt
            c0(tj,tk) = c1(tj,tk)
            d0(tj,tk) = d1(tj,tk)
            e0(tj,tk) = e1(tj,tk)

            a1(tj,tk) = a(j,k,i)
            b1(tj,tk) = b(j,k,i)
            c1(tj,tk) = c(j,k,i)
            d1(tj,tk) = d(j,k,i)
            e1(tj,tk) = e(j,k,i)

            r        = 1.0d0/(b1(tj,tk)-a1(tj,tk)*c0(tj,tk))
            d1(tj,tk)= r*(d1(tj,tk)-a1(tj,tk)*d0(tj,tk))
            e1(tj,tk)= r*(e1(tj,tk)-a1(tj,tk)*e0(tj,tk))
            c1(tj,tk)= r*c1(tj,tk)

            e(j,k,i) = e1(tj,tk)
            d(j,k,i) = d1(tj,tk)
            c(j,k,i) = c1(tj,tk)
        enddo

        do i=n_row_rt-1,2,-1
            c0(tj,tk) = c(j,k,i)
            d0(tj,tk) = d(j,k,i)
            e0(tj,tk) = e(j,k,i)

            d0(tj,tk) = d0(tj,tk)-c0(tj,tk)*d1(tj,tk)
            e0(tj,tk) = e0(tj,tk)-c0(tj,tk)*e1(tj,tk) 
            d1(tj,tk) = d0(tj,tk)
            e1(tj,tk) = e0(tj,tk)
            d(j,k,i)  = d0(tj,tk)
            e(j,k,i)  = e0(tj,tk)
        enddo

        a1(tj,tk) = a(j,k,1)
        b1(tj,tk) = b(j,k,1)
        c1(tj,tk) = c(j,k,1)
        d1(tj,tk) = d(j,k,1)
        e1(tj,tk) = e(j,k,n_row_rt)

        d1(tj,tk) = (d1(tj,tk)-a1(tj,tk)*d(j,k,n_row_rt)-c1(tj,tk)*d0(tj,tk))/(b1(tj,tk)+a1(tj,tk)*e1(tj,tk)+c1(tj,tk)*e0(tj,tk)) ! e0(tj,tk) = e(j,k,2)
        d(j,k,1) = d1(tj,tk)

        do i=2,n_row_rt
            e1(tj,tk) = e(j,k,i)
            d(j,k,i) = d(j,k,i) + d1(tj,tk)*e1(tj,tk)
        enddo

    end subroutine tdma_cycl_many_cuda

    subroutine alltoall_kforward(umpi, uimpi, n1, n2, n3, mpi_comm, nprocs, sendbuf, recvbuf)
        double precision, device ::  umpi(1:n1,1:n2,1:n3)
        double precision, device :: uimpi(1:(n1/nprocs),1:n2,1:(n3*nprocs))
  
        double precision, device :: sendbuf(n1*n3*n2)
        double precision, device :: recvbuf(n1*n3*n2)

        double precision :: temp

        type(dim3) :: blocks, threads
        
        integer  :: mpi_comm
        integer  :: i,j,k,kblk,indexbuf
        integer  :: n1, n2, n3, nprocs
        integer  :: n1blksize, ntotalsize, nblksize
        integer  :: request, ierr
        integer  :: kblk_block, i_block, j_block
  
        n1blksize  = n1/nprocs
        ntotalsize = n1*n2*n3
        nblksize   = ntotalsize/nprocs

        !$acc parallel loop collapse(4) private(kblk_block, i_block, j_block, indexbuf)
        do kblk=1, nprocs
        do i   =1, n1blksize
        do j   =1, n2
        do k   =1, n3
            kblk_block=(kblk-1)*(n1blksize)*(n2)*(n3)
               i_block=(i   -1)            *(n2)*(n3)
               j_block=(j   -1)                 *(n3)
            
            indexbuf = k+j_block+i_block+kblk_block
            sendbuf(indexbuf)=umpi(i+(kblk-1)*n1blksize,j,k)
        enddo
        enddo
        enddo
        enddo
        !$acc end parallel

        ! block_in_x_pascal=n1blksize/BLOCK_DIM_X_PASCAL
        ! block_in_y_pascal=n2/BLOCK_DIM_Y_PASCAL

        ! threads = dim3(BLOCK_DIM_X_PASCAL, BLOCK_DIM_Y_PASCAL, 1)
        ! blocks  = dim3(block_in_x_pascal, block_in_y_pascal, n3)

        ! !write(*,*) threads, blocks

        ! call alltoall_kforward_detach<<<blocks, threads>>>(umpi, sendbuf, n1blksize, n1, n2, n3, nprocs)

        !----- alltoall communication of sendbuf to recvbuf
        ierr = cudaDeviceSynchronize()
        call MPI_ALLTOALL(sendbuf, nblksize, MPI_DOUBLE_PRECISION, &
                          recvbuf, nblksize, MPI_DOUBLE_PRECISION, &
                          mpi_comm, ierr)
        ierr = cudaDeviceSynchronize()

        !$acc parallel loop collapse(4) private(kblk_block, i_block, j_block, indexbuf, temp)
        do kblk=1, nprocs
        do i   =1, n1blksize
        do j   =1, n2
        do k   =1, n3
            kblk_block=(kblk-1)*(n1blksize)*(n2)*(n3)
               i_block=(i   -1)            *(n2)*(n3)
               j_block=(j   -1)                 *(n3)

            indexbuf = k+j_block+i_block+kblk_block
            uimpi(i,j,k+(kblk-1)*(n3)) = recvbuf(indexbuf)
        enddo
        enddo
        enddo
        enddo
        !$acc end parallel

        ! block_in_x_pascal=n1blksize/BLOCK_DIM_X_PASCAL
        ! block_in_y_pascal=n2/BLOCK_DIM_Y_PASCAL

        ! threads = dim3(BLOCK_DIM_X_PASCAL, BLOCK_DIM_Y_PASCAL, 1)
        ! blocks  = dim3(block_in_x_pascal, block_in_y_pascal, n3)

        !call alltoall_kforward_unite<<<blocks, threads>>>(recvbuf, uimpi, n1blksize, n1, n2, n3, nprocs)
  
    end subroutine alltoall_kforward


    attributes(global) subroutine alltoall_kforward_detach(umpi, sendbuf, n1blksize, n1, n2, n3, nprocs)

        implicit none

        integer, value, intent(in)  :: n1, n2, n3, nprocs, n1blksize
        double precision, device, intent(in) :: umpi(1:n1,1:n2,1:n3)
        double precision, device, intent(out) :: sendbuf(n1*n3*n2)

        integer :: i, j, k, kblk
        integer :: kblk_block, i_block, j_block, indexbuf

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        do kblk=1, nprocs
            kblk_block=(kblk-1)*(n1blksize)*(n2)*(n3)
               i_block=(i   -1)            *(n2)*(n3)
               j_block=(j   -1)                 *(n3)
            
            indexbuf = k+j_block+i_block+kblk_block
            sendbuf(indexbuf)=umpi(i+(kblk-1)*n1blksize,j,k)
        enddo

    end subroutine alltoall_kforward_detach

    attributes(global) subroutine alltoall_kforward_unite(recvbuf, uimpi, n1blksize, n1, n2, n3, nprocs)

        implicit none

        integer, value, intent(in)  :: n1, n2, n3, nprocs, n1blksize
        double precision, device, intent(in) :: recvbuf(n1*n3*n2)
        double precision, device, intent(out) :: uimpi(1:(n1/nprocs),1:n2,1:(n3*nprocs))

        integer :: i, j, k, kblk
        integer :: kblk_block, i_block, j_block, indexbuf

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        do kblk=1, nprocs
            kblk_block=(kblk-1)*(n1blksize)*(n2)*(n3)
               i_block=(i   -1)            *(n2)*(n3)
               j_block=(j   -1)                 *(n3)

            indexbuf = k+j_block+i_block+kblk_block
            uimpi(i,j,k+(kblk-1)*(n3)) = recvbuf(indexbuf)
        enddo

    end subroutine alltoall_kforward_unite

    subroutine alltoall_kbackward(umpi, uimpi, n1, n2, n3, mpi_comm, nprocs, sendbuf, recvbuf)
        double precision, device ::  umpi(1:n1,1:n2,1:n3)
        double precision, device :: uimpi(1:(n1/nprocs),1:n2,1:(n3*nprocs))
  
        double precision, device :: sendbuf(n1*n3*n2)
        double precision, device :: recvbuf(n1*n3*n2)

        double precision :: temp

        type(dim3) :: blocks, threads
        
        integer  :: mpi_comm
        integer  :: i,j,k,kblk,indexbuf
        integer  :: n1, n2, n3, nprocs
        integer  :: n1blksize, ntotalsize, nblksize
        integer  :: request, ierr
        integer  :: kblk_block, i_block, j_block
  
        n1blksize  = n1/nprocs
        ntotalsize = n1*n2*n3
        nblksize   = ntotalsize/nprocs

        !$acc parallel loop collapse(4) private(kblk_block, i_block, j_block, indexbuf, temp)
        do kblk=1, nprocs
        do i   =1, n1blksize
        do j   =1, n2
        do k   =1, n3
            kblk_block=(kblk-1)*(n1blksize)*(n2)*(n3)
               i_block=(i   -1)            *(n2)*(n3)
               j_block=(j   -1)                 *(n3)

            indexbuf = k+j_block+i_block+kblk_block
            sendbuf(indexbuf)= uimpi(i,j,k+(kblk-1)*(n3)) 
        enddo
        enddo
        enddo
        enddo
        !$acc end parallel

        ! block_in_x_pascal=n1blksize/BLOCK_DIM_X_PASCAL
        ! block_in_y_pascal=n2/BLOCK_DIM_Y_PASCAL

        ! threads = dim3(BLOCK_DIM_X_PASCAL, BLOCK_DIM_Y_PASCAL, 1)
        ! blocks  = dim3(block_in_x_pascal, block_in_y_pascal, n3)

        !call alltoall_kbackward_detach<<<blocks, threads>>>(uimpi, sendbuf, n1blksize, n1, n2, n3, nprocs)

        !----- alltoall communication of sendbuf to recvbuf
        ierr = cudaDeviceSynchronize()
        call MPI_ALLTOALL(sendbuf, nblksize, MPI_DOUBLE_PRECISION, &
                          recvbuf, nblksize, MPI_DOUBLE_PRECISION, &
                          mpi_comm, ierr)
        ierr = cudaDeviceSynchronize()

        !$acc parallel loop collapse(4) private(kblk_block, i_block, j_block, indexbuf)
        do kblk=1, nprocs
        do i   =1, n1blksize
        do j   =1, n2
        do k   =1, n3
            kblk_block=(kblk-1)*(n1blksize)*(n2)*(n3)
               i_block=(i   -1)            *(n2)*(n3)
               j_block=(j   -1)                 *(n3)
            
            indexbuf = k+j_block+i_block+kblk_block
            umpi(i+(kblk-1)*n1blksize,j,k)=recvbuf(indexbuf)
        enddo
        enddo
        enddo
        enddo
        !$acc end parallel

        ! block_in_x_pascal=n1blksize/BLOCK_DIM_X_PASCAL
        ! block_in_y_pascal=n2/BLOCK_DIM_Y_PASCAL

        ! threads = dim3(BLOCK_DIM_X_PASCAL, BLOCK_DIM_Y_PASCAL, 1)
        ! blocks  = dim3(block_in_x_pascal, block_in_y_pascal, n3)

        !call alltoall_kbackward_unite<<<blocks, threads>>>(recvbuf, umpi, n1blksize, n1, n2, n3, nprocs)

  
    end subroutine alltoall_kbackward

    attributes(global) subroutine alltoall_kbackward_detach(uimpi, sendbuf, n1blksize, n1, n2, n3, nprocs)

        implicit none

        integer, value, intent(in)  :: n1, n2, n3, nprocs, n1blksize
        double precision, device, intent(in) :: uimpi(1:(n1/nprocs),1:n2,1:(n3*nprocs))
        double precision, device, intent(out) :: sendbuf(n1*n3*n2)

        integer :: i, j, k, kblk
        integer :: kblk_block, i_block, j_block, indexbuf

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        do kblk=1, nprocs
            kblk_block=(kblk-1)*(n1blksize)*(n2)*(n3)
               i_block=(i   -1)            *(n2)*(n3)
               j_block=(j   -1)                 *(n3)

            indexbuf = k+j_block+i_block+kblk_block

            sendbuf(indexbuf)= uimpi(i,j,k+(kblk-1)*(n3))
        enddo

    end subroutine alltoall_kbackward_detach

    attributes(global) subroutine alltoall_kbackward_unite(recvbuf, umpi, n1blksize, n1, n2, n3, nprocs)

        implicit none

        integer, value, intent(in)  :: n1, n2, n3, nprocs, n1blksize
        double precision, device, intent(in) :: recvbuf(n1*n3*n2)
        double precision, device, intent(out) :: umpi(1:n1,1:n2,1:n3)

        integer :: i, j, k, kblk
        integer :: kblk_block, i_block, j_block, indexbuf

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        do kblk=1, nprocs
             kblk_block=(kblk-1)*(n1blksize)*(n2)*(n3)
                i_block=(i   -1)            *(n2)*(n3)
                j_block=(j   -1)                 *(n3)

            indexbuf = k+j_block+i_block+kblk_block

            umpi(i+(kblk-1)*n1blksize,j,k)=recvbuf(indexbuf)
        enddo

    end subroutine alltoall_kbackward_unite

    attributes(global) subroutine PaScaL_TDMA_many_modified_Thomas_nonbank_cuda(a, b, c, d, a_rd, b_rd, c_rd, d_rd, n_row)

        implicit none

        integer, value, intent(in)      :: n_row
        double precision, device, intent(inout) :: a(:,:,:), c(:,:,:), d(:,:,:)
        double precision, device, intent(in)    :: b(:,:,:)
        double precision, device, intent(inout)   :: a_rd(:,:,:), b_rd(:,:,:), c_rd(:,:,:), d_rd(:,:,:)

        ! Temporary variables for computation
        integer :: i, j, k
        integer :: ti, tj, tk
        double precision :: r

        j = (blockidx%x-1) * blockdim%x + threadidx%x
        k = (blockidx%y-1) * blockdim%y + threadidx%y


        a(j,k,1) = a(j,k,1) / b(j,k,1)
        c(j,k,1) = c(j,k,1) / b(j,k,1)
        d(j,k,1) = d(j,k,1) / b(j,k,1)

        a(j,k,2) = a(j,k,2) / b(j,k,2)
        c(j,k,2) = c(j,k,2) / b(j,k,2)
        d(j,k,2) = d(j,k,2) / b(j,k,2)

        do i=3, n_row
            r    =    1.0d0/(b(j,k,i)-a(j,k,i)*c(j,k,i-1))
            d(j,k,i) =  r*(d(j,k,i)-a(j,k,i)*d(j,k,i-1))
            c(j,k,i) =  r*c(j,k,i)
            a(j,k,i) = -r*a(j,k,i)*a(j,k,i-1)
        enddo

        ! The modified Thomas algorithm : elimination of upper diagonal elements.
        do i=n_row-2, 2, -1
            d(j,k,i) = d(j,k,i)-c(j,k,i)*d(j,k,i+1)
            a(j,k,i) = a(j,k,i)-c(j,k,i)*a(j,k,i+1)
            c(j,k,i) =-c(j,k,i)*c(j,k,i+1)
        enddo

        r = 1.0d0/(1.0d0-a(j,k,2)*c(j,k,1))
        d(j,k,1) =  r*(d(j,k,1)-c(j,k,1)*d(j,k,2))
        a(j,k,1) =  r*a(j,k,1)
        c(j,k,1) = -r*c(j,k,1)*c(j,k,2)

        ! Construct many reduced tridiagonal systems per each rank. Each process has two rows of reduced systems.
        a_rd(j,k,1) = a(j,k,1); a_rd(j,k,2) = a(j,k,n_row)
        b_rd(j,k,1) = 1.0d0   ; b_rd(j,k,2) = 1.0d0
        c_rd(j,k,1) = c(j,k,1); c_rd(j,k,2) = c(j,k,n_row)
        d_rd(j,k,1) = d(j,k,1); d_rd(j,k,2) = d(j,k,n_row)

    end subroutine PaScaL_TDMA_many_modified_Thomas_nonbank_cuda

    attributes(global) subroutine PaScaL_TDMA_many_update_solution_nonbank_cuda(a, c, d, d_rd, n_row)

        implicit none

        integer, value, intent(in)      :: n_row
        double precision, device, intent(in)    :: a(:,:,:), c(:,:,:), d_rd(:,:,:)
        double precision, device, intent(inout) :: d(:,:,:)

        integer :: i, j, k


        j = (blockidx%x-1) * blockdim%x + threadidx%x
        k = (blockidx%y-1) * blockdim%y + threadidx%y

        d(j,k,1)       = d_rd(j,k,1)
        d(j,k,n_row)   = d_rd(j,k,2)

        do i=2,n_row-1
            d(j,k,i) = d(j,k,i)-a(j,k,i)*d(j,k,1)-c(j,k,i)*d(j,k,n_row)
        enddo

    end subroutine PaScaL_TDMA_many_update_solution_nonbank_cuda

    attributes(global) subroutine tdma_many_nonbank_cuda(a, b, c, d, n1)

        implicit none

        integer, value, intent(in)              :: n1
        double precision, device, intent(in)    :: a(:,:,:), b(:,:,:)
        double precision, device, intent(inout) :: c(:,:,:), d(:,:,:)
        
        integer :: i, j, k
        integer :: tj, tk
        double precision :: r

        j = (blockidx%x-1) * blockdim%x + threadidx%x
        k = (blockidx%y-1) * blockdim%y + threadidx%y


        d(j,k,1)=d(j,k,1)/b(j,k,1)
        c(j,k,1)=c(j,k,1)/b(j,k,1)

        do i=2,n1

            r=1.0d0/(b(j,k,i)-a(j,k,i)*c(j,k,i-1))
            d(j,k,i)=r*(d(j,k,i)-a(j,k,i)*d(j,k,i-1))
            c(j,k,i)=r*(c(j,k,i))

        enddo

        do i=n1-1,1,-1

            d(j,k,i)=d(j,k,i)-c(j,k,i)*d(j,k,i+1)

        enddo

    end subroutine tdma_many_nonbank_cuda

    attributes(global) subroutine tdma_cycl_many_nonbank_cuda(a, b, c, d, e, n1)

        implicit none

        integer, value, intent(in)      :: n1
        double precision, device, intent(in)    :: a(:,:,:), b(:,:,:)
        double precision, device, intent(inout) :: c(:,:,:), d(:,:,:), e(:,:,:)

        integer :: i, j, k
        integer :: tj, tk
        double precision :: r

        j = (blockidx%x-1) * blockdim%x + threadidx%x
        k = (blockidx%y-1) * blockdim%y + threadidx%y

        tj = threadidx%x
        tk = threadidx%y

        do i=1,n1
            e(j,k,i)  = 0.0d0
        enddo
        e(j,k,2)  = -a(j,k,2)
        e(j,k,n1) = -c(j,k,n1)

        d(j,k,2)  = d(j,k,2)/b(j,k,2)
        c(j,k,2)  = c(j,k,2)/b(j,k,2)
        e(j,k,2)  = e(j,k,2)/b(j,k,2)

        do i=3,n1
    
            r = 1.0d0/(b(j,k,i)-a(j,k,i)*c(j,k,i-1))
            d(j,k,i)=r*(d(j,k,i)-a(j,k,i)*d(j,k,i-1))
            e(j,k,i)=r*(e(j,k,i)-a(j,k,i)*e(j,k,i-1))
            c(j,k,i)=r*(c(j,k,i))

        enddo

        do i=n1-1,2,-1

            d(j,k,i)=d(j,k,i)-c(j,k,i)*d(j,k,i+1)
            e(j,k,i)=e(j,k,i)-c(j,k,i)*e(j,k,i+1)

        enddo


        d(j,k,1)=(d(j,k,1)-a(j,k,1)*d(j,k,n1)-c(j,k,1)*d(j,k,2))/(b(j,k,1)+a(j,k,1)*e(j,k,n1)+c(j,k,1)*e(j,k,2))

        do i=2,n1
            d(j,k,i) = d(j,k,i) + d(j,k,1)*e(j,k,i)
        enddo

    end subroutine tdma_cycl_many_nonbank_cuda

end module PaScaL_TDMA_cuda